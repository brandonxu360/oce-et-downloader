{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0b57e0",
   "metadata": {},
   "source": [
    "# Agricultural Fields Data Request\n",
    "\n",
    "Data was requested with the following specifications:\n",
    "\n",
    "* Years: 2008-2024\n",
    "* Months: 4, 5, 6, 7, 8, 9\n",
    "* Type: Remote Sensing:\n",
    "    * **Dataset**: Landsat 5/7/8/9 SR – 30 **Variable**: NDVI, MSAVI, NDWI \n",
    "    * **Dataset**: Open ET – 30m – Monthly **Variable**: ETa: eeMETRIC, geeSEBAL, DISALEXI\n",
    "    * **Dataset**: Sentinel 2 SR – 10m – 5day **Variable**: NDVI, MSAVI, NDWI, NDRE, BSI\n",
    " \n",
    "Variable names:\n",
    "L-NDVI, L-MSAVI, L-NDWI,\n",
    "eeMETRIC, geeSEBAL, DISALEXI\n",
    "S-NDVI, S-MSAVI, S-NDWI, S-NDRE, S-BSI\n",
    "\n",
    "*L is appended in front of the variables generated with Landsat and an S in front of those generated with Sentinel. Sentinel will not have data prior to 2017 or so.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14655860",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AUTHENTICATION\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}\n",
    "\n",
    "# LOGGING\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, # INFO for useful info, DEBUG for uglier, verbose info\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"climateengine.scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927bd92",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "### Datasets\n",
    "Available datasets from ClimateEngine API: https://docs.climateengine.org/docs/build/html/datasets.html\n",
    "\n",
    "API Parameters:\n",
    "* Landsat 5/7/8/9 SR - 30m: LANDSAT_SR\n",
    "* OpenET - 30m - Monthly: OPENET_CONUS\n",
    "* Sentinel 2 SR - 10m - 5day: SENTINEL2_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2974937",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['LANDSAT_SR', 'OPENET_CONUS', 'SENTINEL2_SR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220cd77",
   "metadata": {},
   "source": [
    "### Variables\n",
    "The API allows us to see the available variables for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_variables_metadata_dataset_variables_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a20e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_utils import synchronous_fetch_with_retry\n",
    "\n",
    "# All requested variables for the dataset of corresponding index\n",
    "VARIABLES = [ \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDWI_Green_NIR_McFeeters', 'NDWI_Green_SWIR_Xu', 'NDWI_Green_SWIR_Hall', 'NDWI_SWIR_Green_Allen'],\n",
    "    ['et_eemetric', 'et_geesebal', 'et_disalexi'], \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDWI_Green_NIR_McFeeters', 'NDWI_Green_SWIR_Xu', 'NDWI_Green_SWIR_Hall', 'NDWI_SWIR_Green_Allen']] # Temporarily remove 'NDWI_NIR_SWIR2'\n",
    "\n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_variables?dataset={dataset}', headers=HEADERS)\n",
    "\n",
    "    api_variables = set(res.get('Data').get('variables'))\n",
    "    missing = set(VARIABLES[i]).difference(api_variables)\n",
    "\n",
    "    if missing:\n",
    "        logger.info(f'{dataset}: ✗ API missing requested variables {missing}')\n",
    "    else:\n",
    "        logger.info(f'{dataset}: ✓ all variables available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9f32",
   "metadata": {},
   "source": [
    "### Dates\n",
    "The API allows us to see the minimum and maximum dates for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_dates_metadata_dataset_dates_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requested year range for the dataset of corresponding index\n",
    "DATES = [\n",
    "    [2008, 2024],\n",
    "    [2008, 2024],\n",
    "    [2017, 2024]]\n",
    "\n",
    "MONTHS = [\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9]]\n",
    "    \n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_dates?dataset={dataset}', headers=HEADERS)\n",
    "    \n",
    "    data = res.get('Data')\n",
    "    date_min = int(data['min'][:4])  # Extract year from date string\n",
    "    date_max = int(data['max'][:4])\n",
    "    req_min, req_max = DATES[i]\n",
    "    available = '✓' if req_min >= date_min and req_max <= date_max else '✗'\n",
    "    logger.info(f'{dataset}: {available} (available: {date_min}-{date_max}, requested: {req_min}-{req_max})') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b7c36",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1125355",
   "metadata": {},
   "source": [
    "### Prepare Agricultural Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AG_FIELDS_URL = 'https://wc.bearhive.duckdns.org/weppcloud/runs/copacetic-note/ag-fields/browse/ag_fields/CSB_2008_2024_Hangman_with_Crop_and_Performance.geojson?raw=true'\n",
    "\n",
    "fields_data = synchronous_fetch_with_retry(AG_FIELDS_URL)\n",
    "\n",
    "# Extract field data\n",
    "fields = []\n",
    "for feature in fields_data['features']:\n",
    "    properties = feature['properties']\n",
    "    field_id = properties.get('field_ID')\n",
    "    geometry = feature['geometry']\n",
    "    \n",
    "    fields.append({\n",
    "        'field_id': field_id,\n",
    "        'geometry': geometry,\n",
    "    })\n",
    "\n",
    "fields_df = pd.DataFrame(fields)\n",
    "logger.info(fields_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb538df",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b574ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import calendar\n",
    "from tqdm.asyncio import tqdm\n",
    "from scrape_utils import asynchronous_fetch_with_retry\n",
    "\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "async def fetch_data(dataset: str, dataset_index: int, session: aiohttp.ClientSession):\n",
    "    tasks = [\n",
    "        asynchronous_fetch_with_retry(\n",
    "            session=session,\n",
    "            url='https://api.climateengine.org/timeseries/native/coordinates',\n",
    "            semaphore=semaphore,\n",
    "            headers=HEADERS,\n",
    "            params={\n",
    "                'dataset': dataset,\n",
    "                'variable': ','.join(VARIABLES[dataset_index]),\n",
    "                'start_date': f'{DATES[dataset_index][0]}-{MONTHS[dataset_index][0]:02d}-01',\n",
    "                'end_date': f'{DATES[dataset_index][1]}-{MONTHS[dataset_index][-1]:02d}-{calendar.monthrange(DATES[dataset_index][1], MONTHS[dataset_index][-1])[1]}',\n",
    "                'area_reducer': 'mean',\n",
    "                'coordinates': json.dumps(row['geometry']['coordinates'])\n",
    "            })\n",
    "        for _, row in fields_df.head(10).iterrows()\n",
    "    ]\n",
    "\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "async with aiohttp.ClientSession(raise_for_status=True, timeout=aiohttp.ClientTimeout(total=600000)) as session:\n",
    "    for i, dataset in enumerate(DATASETS):\n",
    "        logger.info(f'{dataset}: starting...')\n",
    "        results = await fetch_data(dataset=dataset, dataset_index=i, session=session)\n",
    "        logger.info(f'{dataset}: fetched {len(results)} results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
