{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0b57e0",
   "metadata": {},
   "source": [
    "# Agricultural Fields Data - Timeseries Endpoint\n",
    "\n",
    "#### Data was requested with the following specifications:\n",
    "\n",
    "* Years: 2008-2024\n",
    "* Months: 4, 5, 6, 7, 8, 9\n",
    "* Type: Remote Sensing:\n",
    "    * **Dataset**: Landsat 5/7/8/9 SR – 30 **Variable**: NDVI, MSAVI, NDWI \n",
    "    * **Dataset**: Open ET – 30m – Monthly **Variable**: ETa: eeMETRIC, geeSEBAL, DISALEXI\n",
    "    * **Dataset**: Sentinel 2 SR – 10m – 5day **Variable**: NDVI, MSAVI, NDWI, NDRE, BSI\n",
    " \n",
    "Variable names:\n",
    "L-NDVI, L-MSAVI, L-NDWI,\n",
    "eeMETRIC, geeSEBAL, DISALEXI\n",
    "S-NDVI, S-MSAVI, S-NDWI, S-NDRE, S-BSI\n",
    "\n",
    "*L is appended in front of the variables generated with Landsat and an S in front of those generated with Sentinel. Sentinel will not have data prior to 2017 or so.*\n",
    "\n",
    "The area reducer of median and temporal reducers (by month) of mean and max are requested.\n",
    "\n",
    "#### Notes\n",
    "The [timeseries endpoint](https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-coordinates) is used in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14655860",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AUTHENTICATION\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}\n",
    "\n",
    "# FILE OUTPUT DIRECTORY\n",
    "OUTPUT_DIR = 'data/output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LOGGING\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, # INFO for useful info, DEBUG for uglier, verbose info\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"climateengine.scraper.timeseries\")\n",
    "\n",
    "LOG_TO_FILE = False # Write logs to file\n",
    "if LOG_TO_FILE:\n",
    "    formatter = logging.Formatter(\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\")\n",
    "    file_handler = logging.FileHandler(f'{OUTPUT_DIR}/et_timeseries_scraper.log')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927bd92",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "### Datasets\n",
    "Available datasets from ClimateEngine API: https://docs.climateengine.org/docs/build/html/datasets.html\n",
    "\n",
    "API Parameters:\n",
    "* Landsat 5/7/8/9 SR - 30m: LANDSAT_SR\n",
    "* OpenET - 30m - Monthly: OPENET_CONUS\n",
    "* Sentinel 2 SR - 10m - 5day: SENTINEL2_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2974937",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['LANDSAT_SR', 'OPENET_CONUS', 'SENTINEL2_SR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220cd77",
   "metadata": {},
   "source": [
    "### Variables\n",
    "The API allows us to see the available variables for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_variables_metadata_dataset_variables_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a20e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_utils import synchronous_fetch_with_retry\n",
    "\n",
    "# All requested variables for the dataset of corresponding index\n",
    "# Most important NDWI variable is NDWI_NIR_SWIR_GAO - vegatation moisture \n",
    "VARIABLES = [ \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao'],\n",
    "    ['et_eemetric', 'et_geesebal', 'et_disalexi'], \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDRE', 'BSI']]\n",
    "\n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_variables?dataset={dataset}', headers=HEADERS)\n",
    "\n",
    "    api_variables = set(res.get('Data').get('variables'))\n",
    "    missing = set(VARIABLES[i]).difference(api_variables)\n",
    "\n",
    "    if missing:\n",
    "        logger.info(f'{dataset}: ✗ API missing requested variables {missing}')\n",
    "    else:\n",
    "        logger.info(f'{dataset}: ✓ all variables available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9f32",
   "metadata": {},
   "source": [
    "### Dates\n",
    "The API allows us to see the minimum and maximum dates for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_dates_metadata_dataset_dates_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requested year range for the dataset of corresponding index indicated by a start and end inclusive\n",
    "YEARS = [\n",
    "    [2008, 2024],\n",
    "    [2008, 2024],\n",
    "    [2017, 2024]]\n",
    "\n",
    "# Explicit enumeration of desired months\n",
    "MONTHS = [\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9]]\n",
    "    \n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_dates?dataset={dataset}', headers=HEADERS)\n",
    "    \n",
    "    data = res.get('Data')\n",
    "    date_min = int(data['min'][:4])  # Extract year from date string\n",
    "    date_max = int(data['max'][:4])\n",
    "    req_min, req_max = YEARS[i]\n",
    "    available = '✓' if req_min >= date_min and req_max <= date_max else '✗'\n",
    "    logger.info(f'{dataset}: {available} (available: {date_min}-{date_max}, requested: {req_min}-{req_max})') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b7c36",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1125355",
   "metadata": {},
   "source": [
    "### Prepare Agricultural Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AG_FIELDS_URL = 'https://wc.bearhive.duckdns.org/weppcloud/runs/copacetic-note/ag-fields/browse/ag_fields/CSB_2008_2024_Hangman_with_Crop_and_Performance.geojson?raw=true'\n",
    "\n",
    "fields_data = synchronous_fetch_with_retry(AG_FIELDS_URL)\n",
    "\n",
    "# Extract field data\n",
    "fields = []\n",
    "for feature in fields_data['features']:\n",
    "    properties = feature['properties']\n",
    "    field_id = properties.get('field_ID')\n",
    "    geometry = feature['geometry']\n",
    "    \n",
    "    fields.append({\n",
    "        'field_id': field_id,\n",
    "        'geometry': geometry,\n",
    "    })\n",
    "\n",
    "fields_df = pd.DataFrame(fields)\n",
    "logger.info(fields_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb538df",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b574ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import calendar\n",
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm\n",
    "from scrape_utils import asynchronous_fetch_with_retry\n",
    "\n",
    "semaphore = asyncio.Semaphore(10) # Restrict the number of active requests to the API\n",
    "CHUNK_SIZE = 2 # Number of fields to collect and save data for at a time\n",
    "INDIVIDUAL_FIELD_DATA_DIRECTORY = f'{OUTPUT_DIR}/individual' # Store the individual field data files\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(INDIVIDUAL_FIELD_DATA_DIRECTORY, exist_ok=True)\n",
    "\n",
    "async def fetch_one_field(\n",
    "    session: aiohttp.ClientSession,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    dataset: str,\n",
    "    variables: list[str],\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    field_id: str | int,\n",
    "    coordinates: list,\n",
    "):\n",
    "    payload = await asynchronous_fetch_with_retry(\n",
    "        session=session,\n",
    "        url='https://api.climateengine.org/timeseries/native/coordinates',\n",
    "        semaphore=semaphore,\n",
    "        headers=HEADERS,\n",
    "        params={\n",
    "            'dataset': dataset,\n",
    "            'variable': ','.join(variables),\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'area_reducer': 'median',\n",
    "            'coordinates': json.dumps(coordinates)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return field_id, payload\n",
    "\n",
    "async def fetch_data(dataset: str, dataset_index: int, session: aiohttp.ClientSession, fields_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fetch the data for all the fields contained in the input dataframe.\n",
    "    \"\"\"\n",
    "    tasks = [\n",
    "        fetch_one_field(\n",
    "            session=session,\n",
    "            semaphore=semaphore,\n",
    "            dataset=dataset,\n",
    "            variables=VARIABLES[dataset_index],\n",
    "            start_date=f'{YEARS[dataset_index][0]}-{MONTHS[dataset_index][0]:02d}-01',\n",
    "            end_date=f'{YEARS[dataset_index][1]}-{MONTHS[dataset_index][-1]:02d}-{calendar.monthrange(YEARS[dataset_index][1], MONTHS[dataset_index][-1])[1]}',\n",
    "            field_id=row[\"field_id\"],\n",
    "            coordinates=row[\"geometry\"][\"coordinates\"],\n",
    "        )\n",
    "        for _, row in fields_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "def convert_results_to_pandas(results) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Convert the responses in the results dictionary into Pandas dataframes\"\"\"\n",
    "    res = {}\n",
    "\n",
    "    for field_id, result in results:\n",
    "        rows = []\n",
    "        \n",
    "        # Each result['Data'] is a list with one item containing the timeseries\n",
    "        if 'Data' in result and len(result['Data']) > 0:\n",
    "            timeseries_data = result['Data'][0]['Data']\n",
    "            \n",
    "            # Each item in timeseries_data is a dict with Date and variable values\n",
    "            for data_point in timeseries_data:\n",
    "                row = {\n",
    "                    'field_id': field_id,\n",
    "                    'date': data_point.get('Date'),\n",
    "                    **{k: v for k, v in data_point.items() if k != 'Date'}  # All variables\n",
    "                }\n",
    "                rows.append(row)\n",
    "        res[field_id] = pd.DataFrame(rows)\n",
    "    return res\n",
    "\n",
    "def get_finished_field_ids(dir: str, dataset: str) -> set[str]:\n",
    "    dataset_dir = Path(dir) / dataset\n",
    "    if not dataset_dir.exists():\n",
    "        return set()\n",
    "    \n",
    "    finished = set()\n",
    "    for field_file_name in dataset_dir.glob('*.parquet'):\n",
    "        field_id = field_file_name.stem\n",
    "        finished.add(field_id)\n",
    "\n",
    "    return finished\n",
    "\n",
    "def process_df(raw_df: pd.DataFrame, dataset_index: int):\n",
    "\n",
    "    # Filter out unwanted months (years should already be capped to the specified range by API)\n",
    "    df = raw_df[pd.to_datetime(raw_df['date']).dt.month.isin(MONTHS[dataset_index])]\n",
    "\n",
    "    # Avoid sentinel value contamination\n",
    "    df = df.replace(-9999, pd.NA)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    keys = ['field_id', 'date', 'year', 'month']\n",
    "    value_cols = [col for col in df.columns if col not in keys]\n",
    "\n",
    "    df[value_cols] = (\n",
    "        df[value_cols]\n",
    "        .apply(pd.to_numeric, errors='coerce')\n",
    "    )\n",
    "    \n",
    "    # Group by field_id and year_month, calculate mean and max\n",
    "    agg_df = df.groupby(['field_id', 'year', 'month'])[value_cols].agg(['mean', 'max']).reset_index()\n",
    "\n",
    "    # Flatten MultiIndex (cleaner column names)\n",
    "    agg_df.columns = [\n",
    "        f\"{col}_{stat}\" if stat else col\n",
    "        for col, stat in agg_df.columns\n",
    "    ]\n",
    "\n",
    "    # Round values to 4 decimals\n",
    "    stat_cols = [c for c in agg_df.columns if c not in ['field_id', 'year', 'month']]\n",
    "    agg_df[stat_cols] = agg_df[stat_cols].round(4)\n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "# Main Loop\n",
    "all_dataset_results = {}\n",
    "async with aiohttp.ClientSession(raise_for_status=True, timeout=aiohttp.ClientTimeout(total=None)) as session:\n",
    "    for i, dataset in enumerate(DATASETS):\n",
    "\n",
    "        logger.info(f'{dataset}: starting...')\n",
    "\n",
    "        # Get the fields that still need processing\n",
    "        finished_field_ids = get_finished_field_ids(INDIVIDUAL_FIELD_DATA_DIRECTORY, dataset)\n",
    "        pending_fields = fields_df[~fields_df['field_id'].isin(finished_field_ids)].head(3)\n",
    "        num_pending_fields = len(pending_fields)\n",
    "        chunks = math.ceil(num_pending_fields / CHUNK_SIZE)\n",
    "        if chunks == 0:\n",
    "            continue\n",
    "\n",
    "        for chunk_num in range(chunks):\n",
    "            start = chunk_num * CHUNK_SIZE\n",
    "            end = min((chunk_num + 1) * CHUNK_SIZE, num_pending_fields)\n",
    "            chunk_df = pending_fields.iloc[start:end]\n",
    "\n",
    "            logger.info(\"%s: chunk %d/%d (%d fields)\", dataset, chunk_num + 1, chunks, len(chunk_df))\n",
    "\n",
    "            results = await fetch_data(dataset=dataset, dataset_index=i, session=session, fields_df=chunk_df)\n",
    "            results = convert_results_to_pandas(results)\n",
    "\n",
    "            dataset_out_dir = Path(INDIVIDUAL_FIELD_DATA_DIRECTORY) / dataset\n",
    "            dataset_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Process and save the data\n",
    "            for field_id, field_df in results.items():\n",
    "                processed_df = process_df(field_df, i)\n",
    "\n",
    "                output_file = dataset_out_dir / f\"{field_id}.parquet\"\n",
    "                processed_df.to_parquet(output_file, engine='pyarrow', compression='snappy', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
