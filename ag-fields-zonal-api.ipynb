{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebe8bb6",
   "metadata": {},
   "source": [
    "# Agricultural Fields Data - Zonal Stats Endpoint\n",
    "\n",
    "#### Notes\n",
    "The [zonal stats endpoint](https://docs.climateengine.org/docs/build/html/zonal_statistics.html#rst-zonal-stats-temporal-dataset-coordinates) is used in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4e219",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AUTHENTICATION\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}\n",
    "\n",
    "# LOGGING\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"climateengine.scraper.zonal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af4900b",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['LANDSAT_SR', 'OPENET_CONUS', 'SENTINEL2_SR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce2b23",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6728e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_utils import synchronous_fetch_with_retry\n",
    "\n",
    "# Variables to calculate statistics for\n",
    "VARIABLES = [\n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao'],\n",
    "    ['et_eemetric', 'et_geesebal', 'et_disalexi'],\n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDRE', 'BSI']\n",
    "]\n",
    "\n",
    "# Verify variables are available\n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(\n",
    "        f'https://api.climateengine.org/metadata/dataset_variables?dataset={dataset}',\n",
    "        headers=HEADERS\n",
    "    )\n",
    "\n",
    "    api_variables = set(res.get('Data').get('variables'))\n",
    "    missing = set(VARIABLES[i]).difference(api_variables)\n",
    "\n",
    "    if missing:\n",
    "        logger.info(f'{dataset}: ✗ API missing requested variables {missing}')\n",
    "    else:\n",
    "        logger.info(f'{dataset}: ✓ all variables available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbd8df",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74124875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year ranges for each dataset\n",
    "YEARS = [\n",
    "    [2008, 2024],  # LANDSAT_SR\n",
    "    [2008, 2024],  # OPENET_CONUS\n",
    "    [2017, 2024]   # SENTINEL2_SR\n",
    "]\n",
    "\n",
    "# Months to process (growing season)\n",
    "MONTHS = [\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9]\n",
    "]\n",
    "\n",
    "# Statistics to calculate for each variable\n",
    "STATISTICS = [\n",
    "    ['mean', 'max'],\n",
    "    ['mean', 'max'],\n",
    "    ['mean', 'max']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12fff8",
   "metadata": {},
   "source": [
    "## Prepare Agricultural Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AG_FIELDS_URL = 'https://wc-prod.bearhive.duckdns.org/weppcloud/runs/copacetic-note/ag-fields/browse/ag_fields/CSB_2008_2024_Hangman_with_Crop_and_Performance.geojson?raw=true'\n",
    "\n",
    "fields_data = synchronous_fetch_with_retry(AG_FIELDS_URL)\n",
    "\n",
    "# Extract field data\n",
    "fields = []\n",
    "for feature in fields_data['features']:\n",
    "    properties = feature['properties']\n",
    "    field_id = properties.get('field_ID')\n",
    "    geometry = feature['geometry']\n",
    "    \n",
    "    fields.append({\n",
    "        'field_id': field_id,\n",
    "        'geometry': geometry,\n",
    "    })\n",
    "\n",
    "fields_df = pd.DataFrame(fields)\n",
    "logger.info(f'Loaded {len(fields_df)} agricultural fields')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166e8a3",
   "metadata": {},
   "source": [
    "## Fetch Monthly Statistics\n",
    "\n",
    "This uses the `/zonal_stats/temporal_dataset/coordinates` endpoint to calculate temporal statistics (mean, max) for each month/year combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c52ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import calendar\n",
    "from tqdm.asyncio import tqdm\n",
    "from scrape_utils import asynchronous_fetch_with_retry\n",
    "\n",
    "semaphore = asyncio.Semaphore(50)\n",
    "\n",
    "async def fetch_data(\n",
    "    dataset: str,\n",
    "    dataset_index: int,\n",
    "    variable: str,\n",
    "    session: aiohttp.ClientSession\n",
    "):  \n",
    "    \"\"\"Fetch all monthly statistics for a specific dataset/variable combination.\"\"\"\n",
    "    tasks = []\n",
    "    task_metadata = []\n",
    "    \n",
    "    for statistic in STATISTICS[dataset_index]:\n",
    "        for year in range(YEARS[dataset_index][0], YEARS[dataset_index][1] + 1):\n",
    "            for month in MONTHS[dataset_index]: \n",
    "                start_date = f'{year}-{month:02d}-01'\n",
    "                end_date = f'{year}-{month:02d}-{calendar.monthrange(year, month)[1]:02d}'\n",
    "\n",
    "                for _, row in fields_df.head(10).iterrows():\n",
    "                    # Store metadata for this task\n",
    "                    task_metadata.append({\n",
    "                        'field_id': row['field_id'],\n",
    "                        'dataset': dataset,\n",
    "                        'variable': variable,\n",
    "                        'statistic': statistic,\n",
    "                        'year': year,\n",
    "                        'month': month\n",
    "                    })\n",
    "                    \n",
    "                    # Create the API call task\n",
    "                    tasks.append(\n",
    "                        asynchronous_fetch_with_retry(\n",
    "                            session=session,\n",
    "                            url='https://api.climateengine.org/zonal_stats/temporal_dataset/coordinates',\n",
    "                            semaphore=semaphore,\n",
    "                            headers=HEADERS,\n",
    "                            params={\n",
    "                                'dataset': dataset,\n",
    "                                'variable': variable,\n",
    "                                'temporal_statistic': statistic,\n",
    "                                'area_reducer': 'median',\n",
    "                                'start_date': start_date,\n",
    "                                'end_date': end_date,\n",
    "                                'coordinates': json.dumps(row['geometry']['coordinates'])\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "    # Gather all results with progress bar\n",
    "    api_responses = await tqdm.gather(*tasks)\n",
    "    \n",
    "    # Combine metadata with API responses\n",
    "    results = []\n",
    "    for metadata, response in zip(task_metadata, api_responses):\n",
    "        # Extract value from API response\n",
    "        value = None\n",
    "        if response and 'Data' in response and len(response['Data']) > 0:\n",
    "            data_dict = response['Data'][0]\n",
    "            \n",
    "            # Try exact match first\n",
    "            if variable in data_dict:\n",
    "                value = data_dict[variable]\n",
    "            else:\n",
    "                # Try with units suffix (e.g., \"et_eemetric (mm)\")\n",
    "                for key in data_dict.keys():\n",
    "                    if key.startswith(variable + ' '):\n",
    "                        value = data_dict[key]\n",
    "                        break\n",
    "        \n",
    "        results.append({\n",
    "            **metadata,  # Unpack metadata\n",
    "            'value': value\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_results_to_dataframe(all_results: list) -> pd.DataFrame:\n",
    "    \"\"\"Convert list of results to a pandas DataFrame with proper structure.\"\"\"\n",
    "    if not all_results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Create column name: variable_statistic (e.g., NDVI_mean, NDVI_max)\n",
    "    df['variable_stat'] = df['variable'] + '_' + df['statistic']\n",
    "    \n",
    "    # Pivot to wide format: one row per field/dataset/year/month, columns for each variable_stat\n",
    "    df_wide = df.pivot_table(\n",
    "        index=['field_id', 'dataset', 'year', 'month'],\n",
    "        columns='variable_stat',\n",
    "        values='value',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    df_wide.columns.name = None\n",
    "    \n",
    "    return df_wide\n",
    "\n",
    "\n",
    "# Main execution\n",
    "all_results = []\n",
    "\n",
    "\n",
    "async with aiohttp.ClientSession(raise_for_status=True, timeout=aiohttp.ClientTimeout(total=None)) as session:\n",
    "    for i, dataset in enumerate(DATASETS):\n",
    "        for variable in VARIABLES[i]:\n",
    "            logger.info(f'{dataset}-{variable}: starting...')\n",
    "            results = await fetch_data(\n",
    "                dataset=dataset,\n",
    "                dataset_index=i,\n",
    "                variable=variable,\n",
    "                session=session\n",
    "            )\n",
    "            logger.info(f'{dataset}-{variable}: fetched {len(results)} results')\n",
    "            \n",
    "            # Add to combined results list\n",
    "            all_results.extend(results)\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "logger.info(f'Processing {len(all_results)} total results...')\n",
    "results_df = convert_results_to_dataframe(all_results)\n",
    "logger.info(f'Created DataFrame with {len(results_df)} rows and {len(results_df.columns)} columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c806a",
   "metadata": {},
   "source": [
    "## Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba80c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of results\n",
    "logger.info(f'Results shape: {results_df.shape}')\n",
    "logger.info(f'Columns: {list(results_df.columns)}')\n",
    "logger.info(f'Datasets: {results_df[\"dataset\"].unique()}')\n",
    "logger.info(f'Date range: {results_df[\"year\"].min()}-{results_df[\"year\"].max()}')\n",
    "\n",
    "# Show sample data\n",
    "display(results_df.head(10))\n",
    "\n",
    "# Check for missing values\n",
    "missing_by_column = results_df.isnull().sum()\n",
    "if missing_by_column.sum() > 0:\n",
    "    logger.info('Missing values by column:')\n",
    "    for col, count in missing_by_column[missing_by_column > 0].items():\n",
    "        logger.info(f'  {col}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc4c62",
   "metadata": {},
   "source": [
    "## Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = 'data/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save combined results\n",
    "combined_file = f'{output_dir}/zonal_stats_monthly_combined.parquet'\n",
    "results_df.to_parquet(combined_file, index=False)\n",
    "logger.info(f'Saved {len(results_df)} rows with {len(results_df.columns)} columns to {combined_file}')\n",
    "\n",
    "# Also save as CSV for easier viewing\n",
    "combined_csv = f'{output_dir}/zonal_stats_monthly_combined.csv'\n",
    "results_df.to_csv(combined_csv, index=False)\n",
    "logger.info(f'Also saved as CSV to {combined_csv}')\n",
    "\n",
    "# Optionally, save separate files per dataset\n",
    "for dataset_name in results_df['dataset'].unique():\n",
    "    dataset_df = results_df[results_df['dataset'] == dataset_name]\n",
    "    output_file = f'{output_dir}/{dataset_name.lower()}_monthly_stats.parquet'\n",
    "    dataset_df.to_parquet(output_file, index=False)\n",
    "    logger.info(f'{dataset_name}: saved {len(dataset_df)} rows to {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
