{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0b57e0",
   "metadata": {},
   "source": [
    "# Agricultural Fields Data - Timeseries Endpoint\n",
    "\n",
    "#### Data was requested with the following specifications:\n",
    "\n",
    "* Years: 2008-2024\n",
    "* Months: 4, 5, 6, 7, 8, 9\n",
    "* Type: Remote Sensing:\n",
    "    * **Dataset**: Landsat 5/7/8/9 SR – 30 **Variable**: NDVI, MSAVI, NDWI \n",
    "    * **Dataset**: Open ET – 30m – Monthly **Variable**: ETa: eeMETRIC, geeSEBAL, DISALEXI\n",
    "    * **Dataset**: Sentinel 2 SR – 10m – 5day **Variable**: NDVI, MSAVI, NDWI, NDRE, BSI\n",
    " \n",
    "Variable names:\n",
    "L-NDVI, L-MSAVI, L-NDWI,\n",
    "eeMETRIC, geeSEBAL, DISALEXI\n",
    "S-NDVI, S-MSAVI, S-NDWI, S-NDRE, S-BSI\n",
    "\n",
    "*L is appended in front of the variables generated with Landsat and an S in front of those generated with Sentinel. Sentinel will not have data prior to 2017 or so.*\n",
    "\n",
    "The area reducer of median and temporal reducers (by month) of mean and max are requested.\n",
    "\n",
    "#### Notes\n",
    "The [timeseries endpoint](https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-coordinates) is used in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14655860",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AUTHENTICATION\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}\n",
    "\n",
    "# LOGGING\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, # INFO for useful info, DEBUG for uglier, verbose info\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"climateengine.scraper.timeseries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927bd92",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "### Datasets\n",
    "Available datasets from ClimateEngine API: https://docs.climateengine.org/docs/build/html/datasets.html\n",
    "\n",
    "API Parameters:\n",
    "* Landsat 5/7/8/9 SR - 30m: LANDSAT_SR\n",
    "* OpenET - 30m - Monthly: OPENET_CONUS\n",
    "* Sentinel 2 SR - 10m - 5day: SENTINEL2_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2974937",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['LANDSAT_SR', 'OPENET_CONUS', 'SENTINEL2_SR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220cd77",
   "metadata": {},
   "source": [
    "### Variables\n",
    "The API allows us to see the available variables for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_variables_metadata_dataset_variables_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a20e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_utils import synchronous_fetch_with_retry\n",
    "\n",
    "# All requested variables for the dataset of corresponding index\n",
    "# Most important NDWI variable is NDWI_NIR_SWIR_GAO - vegatation moisture \n",
    "VARIABLES = [ \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDWI_Green_NIR_McFeeters', 'NDWI_Green_SWIR_Xu', 'NDWI_Green_SWIR_Hall', 'NDWI_SWIR_Green_Allen'],\n",
    "    ['et_eemetric', 'et_geesebal', 'et_disalexi'], \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDWI_Green_NIR_McFeeters', 'NDWI_Green_SWIR_Xu', 'NDWI_Green_SWIR_Hall', 'NDWI_SWIR_Green_Allen']] # Temporarily remove 'NDWI_NIR_SWIR2'\n",
    "\n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_variables?dataset={dataset}', headers=HEADERS)\n",
    "\n",
    "    api_variables = set(res.get('Data').get('variables'))\n",
    "    missing = set(VARIABLES[i]).difference(api_variables)\n",
    "\n",
    "    if missing:\n",
    "        logger.info(f'{dataset}: ✗ API missing requested variables {missing}')\n",
    "    else:\n",
    "        logger.info(f'{dataset}: ✓ all variables available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9f32",
   "metadata": {},
   "source": [
    "### Dates\n",
    "The API allows us to see the minimum and maximum dates for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_dates_metadata_dataset_dates_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requested year range for the dataset of corresponding index\n",
    "DATES = [\n",
    "    [2008, 2024],\n",
    "    [2008, 2024],\n",
    "    [2017, 2024]]\n",
    "\n",
    "MONTHS = [\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9]]\n",
    "    \n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_dates?dataset={dataset}', headers=HEADERS)\n",
    "    \n",
    "    data = res.get('Data')\n",
    "    date_min = int(data['min'][:4])  # Extract year from date string\n",
    "    date_max = int(data['max'][:4])\n",
    "    req_min, req_max = DATES[i]\n",
    "    available = '✓' if req_min >= date_min and req_max <= date_max else '✗'\n",
    "    logger.info(f'{dataset}: {available} (available: {date_min}-{date_max}, requested: {req_min}-{req_max})') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b7c36",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1125355",
   "metadata": {},
   "source": [
    "### Prepare Agricultural Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AG_FIELDS_URL = 'https://wc.bearhive.duckdns.org/weppcloud/runs/copacetic-note/ag-fields/browse/ag_fields/CSB_2008_2024_Hangman_with_Crop_and_Performance.geojson?raw=true'\n",
    "\n",
    "fields_data = synchronous_fetch_with_retry(AG_FIELDS_URL)\n",
    "\n",
    "# Extract field data\n",
    "fields = []\n",
    "for feature in fields_data['features']:\n",
    "    properties = feature['properties']\n",
    "    field_id = properties.get('field_ID')\n",
    "    geometry = feature['geometry']\n",
    "    \n",
    "    fields.append({\n",
    "        'field_id': field_id,\n",
    "        'geometry': geometry,\n",
    "    })\n",
    "\n",
    "fields_df = pd.DataFrame(fields)\n",
    "logger.info(fields_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb538df",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b574ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import calendar\n",
    "from tqdm.asyncio import tqdm\n",
    "from scrape_utils import asynchronous_fetch_with_retry\n",
    "\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "async def fetch_data(dataset: str, dataset_index: int, session: aiohttp.ClientSession):\n",
    "    tasks = [\n",
    "        asynchronous_fetch_with_retry(\n",
    "            session=session,\n",
    "            url='https://api.climateengine.org/timeseries/native/coordinates',\n",
    "            semaphore=semaphore,\n",
    "            headers=HEADERS,\n",
    "            params={\n",
    "                'dataset': dataset,\n",
    "                'variable': ','.join(VARIABLES[dataset_index]),\n",
    "                'start_date': f'{DATES[dataset_index][0]}-{MONTHS[dataset_index][0]:02d}-01',\n",
    "                'end_date': f'{DATES[dataset_index][1]}-{MONTHS[dataset_index][-1]:02d}-{calendar.monthrange(DATES[dataset_index][1], MONTHS[dataset_index][-1])[1]}',\n",
    "                'area_reducer': 'median',\n",
    "                'coordinates': json.dumps(row['geometry']['coordinates'])\n",
    "            })\n",
    "        for _, row in fields_df.head(10).iterrows()\n",
    "    ]\n",
    "\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "def convert_results_to_pandas(results) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    # Order of asyncio results is preserved which allows us to determine field ID\n",
    "    # https://docs.python.org/3/library/asyncio-task.html#running-tasks-concurrently\n",
    "    for i, result in enumerate(results):\n",
    "        field_id = fields_df.iloc[i]['field_id']\n",
    "        \n",
    "        # Each result['Data'] is a list with one item containing the timeseries\n",
    "        if 'Data' in result and len(result['Data']) > 0:\n",
    "            timeseries_data = result['Data'][0]['Data']\n",
    "            \n",
    "            # Each item in timeseries_data is a dict with Date and variable values\n",
    "            for data_point in timeseries_data:\n",
    "                row = {\n",
    "                    'field_id': field_id,\n",
    "                    'date': data_point.get('Date'),\n",
    "                    **{k: v for k, v in data_point.items() if k != 'Date'}  # All variables\n",
    "                }\n",
    "                rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "    \n",
    "# Main Loop\n",
    "all_dataset_results = {}\n",
    "async with aiohttp.ClientSession(raise_for_status=True, timeout=aiohttp.ClientTimeout(total=None)) as session:\n",
    "    for i, dataset in enumerate(DATASETS):\n",
    "\n",
    "        # Fetch dataset data\n",
    "        logger.info(f'{dataset}: starting...')\n",
    "        results = await fetch_data(dataset=dataset, dataset_index=i, session=session)\n",
    "        logger.info(f'{dataset}: fetched {len(results)} results')\n",
    "\n",
    "        # Write dataset data to dataframe\n",
    "        all_dataset_results[dataset] = convert_results_to_pandas(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677b746",
   "metadata": {},
   "source": [
    "### Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each dataset's DataFrame to a separate parquet file\n",
    "import os\n",
    "\n",
    "output_dir = 'data/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name, df in all_dataset_results.items():\n",
    "    output_file = f'{output_dir}/{dataset_name.lower()}_timeseries.parquet'\n",
    "    df.to_parquet(output_file, engine='pyarrow', compression='snappy', index=False)\n",
    "    logger.info(f'{dataset_name}: saved {len(df)} rows to {output_file}')\n",
    "    \n",
    "logger.info(f'All datasets saved to {output_dir}/')\n",
    "\n",
    "# Save a combined file with all datasets\n",
    "combined_df = pd.concat([\n",
    "    df.assign(dataset=dataset_name) \n",
    "    for dataset_name, df in all_dataset_results.items()\n",
    "], ignore_index=True)\n",
    "\n",
    "combined_file = f'{output_dir}/all_datasets_combined.parquet'\n",
    "combined_df.to_parquet(combined_file, engine='pyarrow', compression='snappy', index=False)\n",
    "logger.info(f'Combined: saved {len(combined_df)} rows to {combined_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
