{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20a4cb2",
   "metadata": {},
   "source": [
    "# Step 0: Setup\n",
    "Configure headers with API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592b7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4d0f0",
   "metadata": {},
   "source": [
    "# Step 1: API Exploration\n",
    "\n",
    "Test the API to infer details regarding how it works and what it accepts.\n",
    "\n",
    "### Fetch Subcatchment Geometries\n",
    "\n",
    "Load the GeoJSON file containing subcatchment polygons with TopazID identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cc406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 555 subcatchments\n",
      "Found 392 unique TopazIDs\n",
      "Sample TopazIDs: [32, 33, 42, 43, 52]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# URL for the subcatchment GeoJSON\n",
    "GEOJSON_URL = 'https://wc-prod.bearhive.duckdns.org/weppcloud/runs/batch;;nasa-roses-2025;;wa-0/disturbed9002_wbt/download/dem/wbt/subcatchments.WGS.geojson'\n",
    "\n",
    "# Fetch and parse the GeoJSON data\n",
    "resp = requests.get(GEOJSON_URL)\n",
    "if resp.ok:\n",
    "    geojson_data = resp.json()\n",
    "    print(f\"Loaded {len(geojson_data['features'])} subcatchments\")\n",
    "    \n",
    "    # Extract unique TopazIDs and their geometries\n",
    "    subcatchments = []\n",
    "    for feature in geojson_data['features']:\n",
    "        topaz_id = feature['properties']['TopazID']\n",
    "        geometry = feature['geometry']\n",
    "        subcatchments.append({'topaz_id': topaz_id, 'geometry': geometry})\n",
    "    \n",
    "    # Create DataFrame and get unique subcatchments\n",
    "    subcatchments_df = pd.DataFrame(subcatchments)\n",
    "    unique_subcatchments = subcatchments_df.groupby('topaz_id').first().reset_index()\n",
    "    print(f\"Found {len(unique_subcatchments)} unique TopazIDs\")\n",
    "    print(f\"Sample TopazIDs: {unique_subcatchments['topaz_id'].head().tolist()}\")\n",
    "else:\n",
    "    print(f'Error {resp.status_code}: {resp.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60004b66",
   "metadata": {},
   "source": [
    "### Check for Disjointed Subcatchments & Test MultiPolygon Support\n",
    "\n",
    "Analyze the data to see if any TopazIDs have multiple polygons, and test if the API supports MultiPolygon geometries.\n",
    "\n",
    "*Spoiler Alert: [It does](https://support.climateengine.org/article/152-formatting-coordinates-for-api-requests), but a separate value is returned for each segment of the MultiPolygon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcbad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of TopazID distribution:\n",
      "Total features: 555\n",
      "Unique TopazIDs: 392\n",
      "Disjointed subcatchments (TopazID with multiple polygons): 89\n",
      "\n",
      "Top 10 TopazIDs with most polygons:\n",
      "topaz_id\n",
      "1343    10\n",
      "1623     8\n",
      "1223     7\n",
      "1672     7\n",
      "152      5\n",
      "1433     5\n",
      "1452     5\n",
      "1392     5\n",
      "73       5\n",
      "1693     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Testing API behavior with multiple polygons...\n",
      "============================================================\n",
      "\n",
      "Test TopazID: 1343\n",
      "Number of polygons: 10\n",
      "✓ SUCCESS: API accepts multiple polygons!\n",
      "  Response status: 200\n",
      "  Number of data items returned: 10\n",
      "  Number of polygons sent: 10\n",
      "\n",
      "✓ API returns SEPARATE data for each polygon\n",
      "  → Need to aggregate results across polygons for disjointed subcatchments\n",
      "\n",
      "  Sample data structure:\n",
      "{\n",
      "  \"Metadata\": {\n",
      "    \"DRI_OBJECTID\": \"[[-123.71241681201863, 47.30913865947637], [-123.71241318476312, 47.30886873208591], [-123.71201631836027, 47.30887119844715], [-123.71201269317766, 47.30860127102055], [-123.71161582874234, 47.308603735984484], [-123.71160133661905, 47.307524026055646], [-123.71199819297385, 47.30752156118429], [-123.71199094318794, 47.306981706188246], [-123.7115940908732, 47.306984171013305], [-123.71158684533792, 47.306444315919016], [-123.71118999701055, 47.30644677932...\n",
      "✓ SUCCESS: API accepts multiple polygons!\n",
      "  Response status: 200\n",
      "  Number of data items returned: 10\n",
      "  Number of polygons sent: 10\n",
      "\n",
      "✓ API returns SEPARATE data for each polygon\n",
      "  → Need to aggregate results across polygons for disjointed subcatchments\n",
      "\n",
      "  Sample data structure:\n",
      "{\n",
      "  \"Metadata\": {\n",
      "    \"DRI_OBJECTID\": \"[[-123.71241681201863, 47.30913865947637], [-123.71241318476312, 47.30886873208591], [-123.71201631836027, 47.30887119844715], [-123.71201269317766, 47.30860127102055], [-123.71161582874234, 47.308603735984484], [-123.71160133661905, 47.307524026055646], [-123.71199819297385, 47.30752156118429], [-123.71199094318794, 47.306981706188246], [-123.7115940908732, 47.306984171013305], [-123.71158684533792, 47.306444315919016], [-123.71118999701055, 47.30644677932...\n"
     ]
    }
   ],
   "source": [
    "API_URL = 'https://api.climateengine.org'\n",
    "\n",
    "# Check for duplicate TopazIDs (indicating disjointed subcatchments)\n",
    "topaz_counts = subcatchments_df['topaz_id'].value_counts()\n",
    "disjointed = topaz_counts[topaz_counts > 1]\n",
    "\n",
    "print(f\"Analysis of TopazID distribution:\")\n",
    "print(f\"Total features: {len(subcatchments_df)}\")\n",
    "print(f\"Unique TopazIDs: {len(topaz_counts)}\")\n",
    "print(f\"Disjointed subcatchments (TopazID with multiple polygons): {len(disjointed)}\")\n",
    "\n",
    "if len(disjointed) > 0:\n",
    "    print(f\"\\nTop 10 TopazIDs with most polygons:\")\n",
    "    print(disjointed.head(10))\n",
    "    \n",
    "    # Test API behavior with multiple polygons\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing API behavior with multiple polygons...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get a TopazID with multiple polygons\n",
    "    test_topaz = disjointed.index[0]\n",
    "    test_geometries = subcatchments_df[subcatchments_df['topaz_id'] == test_topaz]['geometry'].tolist()\n",
    "    \n",
    "    print(f\"\\nTest TopazID: {test_topaz}\")\n",
    "    print(f\"Number of polygons: {len(test_geometries)}\")\n",
    "    \n",
    "    # Test API call with multiple polygons\n",
    "    url = f'{API_URL}/timeseries/native/coordinates'\n",
    "    \n",
    "    # Format: list of outer rings for each polygon\n",
    "    # According to docs: [[polygon1_coords], [polygon2_coords], ...]\n",
    "    # Docs: https://support.climateengine.org/article/152-formatting-coordinates-for-api-requests\n",
    "    all_polygon_coords = []\n",
    "    for geom in test_geometries:\n",
    "        if geom['type'] == 'Polygon':\n",
    "            all_polygon_coords.append(geom['coordinates'][0])  # Outer ring only\n",
    "    \n",
    "    params = {\n",
    "        'dataset': 'OPENET_CONUS',\n",
    "        'variable': 'et_ensemble_mad',\n",
    "        'start_date': '2020-01-01',\n",
    "        'end_date': '2020-01-31',  # Just one month for testing\n",
    "        'coordinates': str(all_polygon_coords),\n",
    "        'area_reducer': 'mean'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, params=params, timeout=60)\n",
    "        if resp.ok:\n",
    "            result = resp.json()\n",
    "            print(\"✓ SUCCESS: API accepts multiple polygons!\")\n",
    "            print(f\"  Response status: {resp.status_code}\")\n",
    "            \n",
    "            # Check how the API returns data for multiple polygons\n",
    "            num_data_items = len(result.get('Data', []))\n",
    "            print(f\"  Number of data items returned: {num_data_items}\")\n",
    "            print(f\"  Number of polygons sent: {len(all_polygon_coords)}\")\n",
    "            \n",
    "            if num_data_items == len(all_polygon_coords):\n",
    "                print(\"\\n✓ API returns SEPARATE data for each polygon\")\n",
    "                print(\"  → Need to aggregate results across polygons for disjointed subcatchments\")\n",
    "                print(\"\\n  Sample data structure:\")\n",
    "                import json\n",
    "                print(json.dumps(result['Data'][0], indent=2)[:500] + \"...\")\n",
    "            elif num_data_items == 1:\n",
    "                print(\"\\n✓ API returns AGGREGATED data across all polygons\")\n",
    "                print(\"  → No additional aggregation needed\")\n",
    "            else:\n",
    "                print(f\"\\n⚠ Unexpected: {num_data_items} data items for {len(all_polygon_coords)} polygons\")\n",
    "        else:\n",
    "            print(f\"✗ FAILED: API returned error {resp.status_code}\")\n",
    "            print(f\"  Error message: {resp.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EXCEPTION: {e}\")\n",
    "        print(\"  Could not determine API behavior\")\n",
    "else:\n",
    "    print(\"\\n✓ No disjointed subcatchments found - all TopazIDs have single polygons\")\n",
    "    print(\"  No need to handle multiple polygons per subcatchment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af140e",
   "metadata": {},
   "source": [
    "### Check for Polygons with Holes (Interior Rings)\n",
    "\n",
    "Analyze whether any subcatchments have interior rings (holes) and test if the API supports them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c37ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of interior rings (holes):\n",
      "Total polygon features: 555\n",
      "Polygons with holes: 3\n",
      "\n",
      "Sample polygons with holes:\n",
      "  TopazID 1673: 4 hole(s)\n",
      "  TopazID 843: 1 hole(s)\n",
      "  TopazID 763: 1 hole(s)\n",
      "\n",
      "============================================================\n",
      "Testing API behavior with polygon holes...\n",
      "============================================================\n",
      "\n",
      "Test TopazID: 1673\n",
      "Number of holes: 4\n",
      "Number of coordinate rings: 5\n",
      "\n",
      "Test 1: Passing OUTER RING ONLY...\n",
      "  ✓ Success - got 1 data items\n",
      "  Sample ET value: 3.7311\n",
      "\n",
      "Test 2: Passing FULL POLYGON (with holes)...\n",
      "  ✓ Success - got 1 data items\n",
      "  Sample ET value: 3.7311\n",
      "\n",
      "Test 2: Passing FULL POLYGON (with holes)...\n",
      "  ✓ Success - got 5 data items\n",
      "  Sample ET value: 3.7311\n",
      "\n",
      "  Comparison:\n",
      "    Outer ring only: 3.7311\n",
      "    With holes:      3.7311\n",
      "    → Values are SAME - API may be ignoring holes\n",
      "\n",
      "============================================================\n",
      "CONCLUSION:\n",
      "⚠ API appears to IGNORE interior rings\n",
      "  This means ET may be OVERESTIMATED for areas with holes\n",
      "============================================================\n",
      "  ✓ Success - got 5 data items\n",
      "  Sample ET value: 3.7311\n",
      "\n",
      "  Comparison:\n",
      "    Outer ring only: 3.7311\n",
      "    With holes:      3.7311\n",
      "    → Values are SAME - API may be ignoring holes\n",
      "\n",
      "============================================================\n",
      "CONCLUSION:\n",
      "⚠ API appears to IGNORE interior rings\n",
      "  This means ET may be OVERESTIMATED for areas with holes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check if any polygons have holes (interior rings)\n",
    "# According to GeoJSON spec: coordinates[0] = outer ring, coordinates[1+] = holes\n",
    "\n",
    "polygons_with_holes = []\n",
    "for idx, row in subcatchments_df.iterrows():\n",
    "    geom = row['geometry']\n",
    "    if geom['type'] == 'Polygon':\n",
    "        # Polygon has holes if len(coordinates) > 1\n",
    "        num_rings = len(geom['coordinates'])\n",
    "        if num_rings > 1:\n",
    "            polygons_with_holes.append({\n",
    "                'topaz_id': row['topaz_id'],\n",
    "                'num_holes': num_rings - 1,\n",
    "                'geometry': geom\n",
    "            })\n",
    "\n",
    "print(f\"Analysis of interior rings (holes):\")\n",
    "print(f\"Total polygon features: {len(subcatchments_df[subcatchments_df['geometry'].apply(lambda g: g['type'] == 'Polygon')])}\")\n",
    "print(f\"Polygons with holes: {len(polygons_with_holes)}\")\n",
    "\n",
    "if len(polygons_with_holes) > 0:\n",
    "    print(f\"\\nSample polygons with holes:\")\n",
    "    for item in polygons_with_holes[:5]:\n",
    "        print(f\"  TopazID {item['topaz_id']}: {item['num_holes']} hole(s)\")\n",
    "    \n",
    "    # Test if API handles holes correctly\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing API behavior with polygon holes...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_poly = polygons_with_holes[0]\n",
    "    print(f\"\\nTest TopazID: {test_poly['topaz_id']}\")\n",
    "    print(f\"Number of holes: {test_poly['num_holes']}\")\n",
    "    print(f\"Number of coordinate rings: {len(test_poly['geometry']['coordinates'])}\")\n",
    "    \n",
    "    # Test 1: Pass only outer ring (current approach)\n",
    "    url = f'{API_URL}/timeseries/native/coordinates'\n",
    "    outer_ring_only = [test_poly['geometry']['coordinates'][0]]\n",
    "    \n",
    "    params1 = {\n",
    "        'dataset': 'OPENET_CONUS',\n",
    "        'variable': 'et_ensemble_mad',\n",
    "        'start_date': '2020-01-01',\n",
    "        'end_date': '2020-01-31',\n",
    "        'coordinates': str(outer_ring_only),\n",
    "        'area_reducer': 'mean'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTest 1: Passing OUTER RING ONLY...\")\n",
    "    try:\n",
    "        resp1 = requests.get(url, headers=HEADERS, params=params1, timeout=60)\n",
    "        if resp1.ok:\n",
    "            result1 = resp1.json()\n",
    "            print(f\"  ✓ Success - got {len(result1.get('Data', []))} data items\")\n",
    "            if 'Data' in result1 and len(result1['Data']) > 0 and 'Data' in result1['Data'][0]:\n",
    "                sample_value = result1['Data'][0]['Data'][0].get('et_ensemble_mad (mm)', 'N/A')\n",
    "                print(f\"  Sample ET value: {sample_value}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Failed: {resp1.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Exception: {e}\")\n",
    "    \n",
    "    # Test 2: Pass full polygon with holes\n",
    "    print(\"\\nTest 2: Passing FULL POLYGON (with holes)...\")\n",
    "    # According to GeoJSON, a polygon with holes is: [[outer], [hole1], [hole2], ...]\n",
    "    # But Climate Engine API expects a list of coordinate rings\n",
    "    # Let's try passing all rings\n",
    "    all_rings = test_poly['geometry']['coordinates']\n",
    "    \n",
    "    params2 = {\n",
    "        'dataset': 'OPENET_CONUS',\n",
    "        'variable': 'et_ensemble_mad',\n",
    "        'start_date': '2020-01-01',\n",
    "        'end_date': '2020-01-31',\n",
    "        'coordinates': str(all_rings),\n",
    "        'area_reducer': 'mean'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp2 = requests.get(url, headers=HEADERS, params=params2, timeout=60)\n",
    "        if resp2.ok:\n",
    "            result2 = resp2.json()\n",
    "            print(f\"  ✓ Success - got {len(result2.get('Data', []))} data items\")\n",
    "            if 'Data' in result2 and len(result2['Data']) > 0 and 'Data' in result2['Data'][0]:\n",
    "                sample_value = result2['Data'][0]['Data'][0].get('et_ensemble_mad (mm)', 'N/A')\n",
    "                print(f\"  Sample ET value: {sample_value}\")\n",
    "            \n",
    "            # Compare results\n",
    "            if resp1.ok and 'Data' in result1 and 'Data' in result2:\n",
    "                val1 = result1['Data'][0]['Data'][0].get('et_ensemble_mad (mm)', 0)\n",
    "                val2 = result2['Data'][0]['Data'][0].get('et_ensemble_mad (mm)', 0)\n",
    "                print(f\"\\n  Comparison:\")\n",
    "                print(f\"    Outer ring only: {val1}\")\n",
    "                print(f\"    With holes:      {val2}\")\n",
    "                if abs(val1 - val2) < 0.01:\n",
    "                    print(f\"    → Values are SAME - API may be ignoring holes\")\n",
    "                else:\n",
    "                    print(f\"    → Values DIFFER - API may be treating each ring separately\")\n",
    "        else:\n",
    "            print(f\"  ✗ Failed: {resp2.status_code} - {resp2.text[:200]}\")\n",
    "            print(f\"    → API may not support polygons with holes in this format\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Exception: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONCLUSION:\")\n",
    "    if not resp2.ok:\n",
    "        print(\"⚠ API likely does NOT support holes - use outer ring only\")\n",
    "        print(\"  This means ET may be OVERESTIMATED for areas with holes\")\n",
    "        print(\"  (calculating ET over the hole areas too)\")\n",
    "    elif resp1.ok and abs(val1 - val2) < 0.01:\n",
    "        print(\"⚠ API appears to IGNORE interior rings\")\n",
    "        print(\"  This means ET may be OVERESTIMATED for areas with holes\")\n",
    "    else:\n",
    "        print(\"✓ API may handle holes correctly\")\n",
    "        print(\"  Need to pass full polygon coordinates including holes\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n✓ No polygons with holes found - all polygons are simple\")\n",
    "    print(\"  No need to worry about interior rings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a785e",
   "metadata": {},
   "source": [
    "### Merge Disjointed Subcatchments\n",
    "\n",
    "If disjointed subcatchments exist, create a proper merged geometry for each TopazID that combines all polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea50127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging disjointed subcatchments...\n",
      "Processed 392 unique TopazIDs\n",
      "\n",
      "Geometry type distribution:\n",
      "geometry\n",
      "Polygon         303\n",
      "MultiPolygon     89\n",
      "Name: count, dtype: int64\n",
      "\n",
      "⚠ 89 TopazIDs have disjointed subcatchments (multiple polygons)\n",
      "  These will be handled by passing all polygon coordinates to the API\n",
      "  and aggregating the results appropriately.\n"
     ]
    }
   ],
   "source": [
    "def merge_polygon_geometries(geometries):\n",
    "    \"\"\"\n",
    "    Merge multiple Polygon geometries into a unified structure for API calls.\n",
    "    Returns a dict with 'type' and 'coordinates' that can be passed to fetch_et_timeseries.\n",
    "    \n",
    "    For multiple polygons, coordinates will be a list of outer rings.\n",
    "    Note: Interior rings (holes) are currently ignored since the Climate Engine API\n",
    "    likely doesn't support them properly. This may cause slight overestimation of ET\n",
    "    for polygons with holes.\n",
    "    \"\"\"\n",
    "    if len(geometries) == 1:\n",
    "        return geometries[0]\n",
    "    \n",
    "    # Collect all polygon outer rings (ignoring holes for now)\n",
    "    all_outer_rings = []\n",
    "    for geom in geometries:\n",
    "        if geom['type'] == 'Polygon':\n",
    "            # coordinates[0] is the outer ring (per GeoJSON spec RFC 7946)\n",
    "            # coordinates[1+] would be holes, which we skip\n",
    "            all_outer_rings.append(geom['coordinates'][0])\n",
    "        elif geom['type'] == 'MultiPolygon':\n",
    "            # Flatten MultiPolygon into separate outer rings\n",
    "            for polygon_coords in geom['coordinates']:\n",
    "                # polygon_coords[0] is the outer ring of this polygon\n",
    "                all_outer_rings.append(polygon_coords[0])\n",
    "    \n",
    "    # Return as a special format that our fetch function can handle\n",
    "    return {\n",
    "        'type': 'MultiPolygon',\n",
    "        'coordinates': all_outer_rings  # List of outer rings only\n",
    "    }\n",
    "\n",
    "# Create properly merged subcatchments\n",
    "print(\"Merging disjointed subcatchments...\")\n",
    "\n",
    "# Group geometries by topaz_id and merge them manually to avoid pandas dict expansion issues\n",
    "merged_data = []\n",
    "for topaz_id, group in subcatchments_df.groupby('topaz_id'):\n",
    "    geometries = group['geometry'].tolist()\n",
    "    merged_geom = merge_polygon_geometries(geometries)\n",
    "    merged_data.append({'topaz_id': topaz_id, 'geometry': merged_geom})\n",
    "\n",
    "unique_subcatchments = pd.DataFrame(merged_data)\n",
    "\n",
    "print(f\"Processed {len(unique_subcatchments)} unique TopazIDs\")\n",
    "\n",
    "# Show statistics\n",
    "geometry_types = unique_subcatchments['geometry'].apply(lambda g: g['type']).value_counts()\n",
    "print(f\"\\nGeometry type distribution:\")\n",
    "print(geometry_types)\n",
    "\n",
    "if 'MultiPolygon' in geometry_types:\n",
    "    multi_count = geometry_types['MultiPolygon']\n",
    "    print(f\"\\n⚠ {multi_count} TopazIDs have disjointed subcatchments (multiple polygons)\")\n",
    "    print(\"  These will be handled by passing all polygon coordinates to the API\")\n",
    "    print(\"  and aggregating the results appropriately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756b87c",
   "metadata": {},
   "source": [
    "# Step 2: Get Available OpenET Variables/Models\n",
    "\n",
    "Query the Climate Engine API to find all available ET variables for the OPENET_CONUS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a1b9678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Request\": {\n",
      "    \"dataset\": \"OPENET_CONUS\",\n",
      "    \"export_format\": null,\n",
      "    \"coll_url\": \"https://support.climateengine.org/article/78-openet\"\n",
      "  },\n",
      "  \"Data\": {\n",
      "    \"variables\": [\n",
      "      \"et_eemetric\",\n",
      "      \"et_sims\",\n",
      "      \"et_ssebop\",\n",
      "      \"et_geesebal\",\n",
      "      \"et_ptjpl\",\n",
      "      \"et_disalexi\",\n",
      "      \"et_ensemble_mad\",\n",
      "      \"et_ensemble_mad_min\",\n",
      "      \"et_ensemble_mad_max\"\n",
      "    ],\n",
      "    \"variable names\": [\n",
      "      \"Evapotranspiration eeMETRIC\",\n",
      "      \"Evapotranspiration SIMS\",\n",
      "      \"Evapotranspiration SSEBop\",\n",
      "      \"Evapotranspiration geeSEBAL\",\n",
      "      \"Evapotranspiration PT-JPL\",\n",
      "      \"Evapotranspiration DisALEXI\",\n",
      "      \"Evapotranspiration Ensemble Median\",\n",
      "      \"Evapotranspiration Ensemble Minimum\",\n",
      "      \"Evapotranspiration Ensemble Maximum\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "API_URL = 'https://api.climateengine.org'\n",
    "\n",
    "# Get available variables for OPENET_CONUS dataset\n",
    "metadata_url = f'{API_URL}/metadata/dataset_variables'\n",
    "params = {'dataset': 'OPENET_CONUS'}\n",
    "\n",
    "resp = requests.get(metadata_url, headers=HEADERS, params=params)\n",
    "if resp.ok:\n",
    "    variables_data = resp.json()\n",
    "    print(json.dumps(variables_data, indent=2))\n",
    "    \n",
    "    # Extract ET model variables\n",
    "    if 'Variables' in variables_data:\n",
    "        et_variables = variables_data['Variables']\n",
    "        print(f\"\\nFound {len(et_variables)} variables\")\n",
    "else:\n",
    "    print(f'Error {resp.status_code}: {resp.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc94fc",
   "metadata": {},
   "source": [
    "## Step 3: Fetch ET Timeseries Data\n",
    "\n",
    "Create a function to fetch monthly ET data for each subcatchment polygon using the Climate Engine timeseries API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e016ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with TopazID: 32\n",
      "Geometry type: MultiPolygon\n",
      "\n",
      "API Response structure:\n",
      "{\n",
      " \"Data\": [\n",
      "  {\n",
      "   \"Data\": [\n",
      "    {\n",
      "     \"Date\": \"2020-01-01\",\n",
      "     \"et_ensemble_mad (mm)\": 4.09895\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-02-01\",\n",
      "     \"et_ensemble_mad (mm)\": 17.7427\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-03-01\",\n",
      "     \"et_ensemble_mad (mm)\": 37.2716\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-04-01\",\n",
      "     \"et_ensemble_mad (mm)\": 78.5382\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-05-01\",\n",
      "     \"et_ensemble_mad (mm)\": 108.2616\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-06-01\",\n",
      "     \"et_ensemble_mad (mm)\": 106.68719999999999\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-07-01\",\n",
      "     \"et_ensemble_mad (mm)\": 121.7559\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-08-01\",\n",
      "     \"et_ensemble_mad (mm)\": 116.9302\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-09-01\",\n",
      "     \"et_ensemble_mad (mm)\": 73.34215\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-10-01\",\n",
      "     \"et_ensemble_mad (mm)\": 29.85235\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-11-01\",\n",
      "     \"et_ensemble_mad (mm)\": 9.31655\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-12-01\",\n",
      "     \"et_ensemble_mad (mm)\": 14.7127\n",
      "    }\n",
      "   ]\n",
      "  }\n",
      " ]\n",
      "}...\n",
      "\n",
      "NoData Stats: {'total_entries': 24, 'nodata_count': 0}\n",
      "\n",
      "API Response structure:\n",
      "{\n",
      " \"Data\": [\n",
      "  {\n",
      "   \"Data\": [\n",
      "    {\n",
      "     \"Date\": \"2020-01-01\",\n",
      "     \"et_ensemble_mad (mm)\": 4.09895\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-02-01\",\n",
      "     \"et_ensemble_mad (mm)\": 17.7427\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-03-01\",\n",
      "     \"et_ensemble_mad (mm)\": 37.2716\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-04-01\",\n",
      "     \"et_ensemble_mad (mm)\": 78.5382\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-05-01\",\n",
      "     \"et_ensemble_mad (mm)\": 108.2616\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-06-01\",\n",
      "     \"et_ensemble_mad (mm)\": 106.68719999999999\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-07-01\",\n",
      "     \"et_ensemble_mad (mm)\": 121.7559\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-08-01\",\n",
      "     \"et_ensemble_mad (mm)\": 116.9302\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-09-01\",\n",
      "     \"et_ensemble_mad (mm)\": 73.34215\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-10-01\",\n",
      "     \"et_ensemble_mad (mm)\": 29.85235\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-11-01\",\n",
      "     \"et_ensemble_mad (mm)\": 9.31655\n",
      "    },\n",
      "    {\n",
      "     \"Date\": \"2020-12-01\",\n",
      "     \"et_ensemble_mad (mm)\": 14.7127\n",
      "    }\n",
      "   ]\n",
      "  }\n",
      " ]\n",
      "}...\n",
      "\n",
      "NoData Stats: {'total_entries': 24, 'nodata_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Create a session with retry configuration\n",
    "def create_session_with_retries(retries=3, backoff_factor=1, status_forcelist=(500, 502, 504)):\n",
    "    \"\"\"\n",
    "    Create a requests session with automatic retry logic.\n",
    "    \n",
    "    Parameters:\n",
    "    - retries: Number of retry attempts (default: 3)\n",
    "    - backoff_factor: Exponential backoff factor (default: 1 -> 1s, 2s, 4s)\n",
    "    - status_forcelist: HTTP status codes to retry on\n",
    "    \n",
    "    Returns:\n",
    "    - Configured requests.Session object\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "        allowed_methods=[\"GET\", \"POST\"],  # Retry on GET and POST\n",
    "        raise_on_status=False  # Don't raise exception on bad status\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "# Create session once to reuse across requests\n",
    "api_session = create_session_with_retries(retries=3, backoff_factor=1)\n",
    "\n",
    "def fetch_et_timeseries(geometry, variable, start_date, end_date, area_reducer='mean'):\n",
    "    \"\"\"\n",
    "    Fetch ET timeseries data for a polygon or multipolygon geometry.\n",
    "    \n",
    "    Parameters:\n",
    "    - geometry: GeoJSON geometry object (Polygon or MultiPolygon)\n",
    "    - variable: ET variable name (e.g., 'et_ensemble_mad')\n",
    "    - start_date: Start date (YYYY-MM-DD format)\n",
    "    - end_date: End date (YYYY-MM-DD format)\n",
    "    - area_reducer: Spatial aggregation method (default: 'mean')\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with:\n",
    "        - 'data': Processed API response with timeseries data, or None if request fails\n",
    "        - 'nodata_stats': {'total_entries': int, 'nodata_count': int}\n",
    "    \n",
    "    Note: For MultiPolygon, the API returns separate data for each polygon.\n",
    "    We aggregate by taking the mean across all polygons for each date.\n",
    "    NoData values (-9999.0) are filtered during aggregation.\n",
    "    Uses automatic retry with exponential backoff via requests session.\n",
    "    \"\"\"\n",
    "    url = f'{API_URL}/timeseries/native/coordinates'\n",
    "    \n",
    "    # Extract coordinates from geometry\n",
    "    if geometry['type'] == 'Polygon':\n",
    "        coordinates = [geometry['coordinates'][0]]  # Outer ring only\n",
    "    elif geometry['type'] == 'MultiPolygon':\n",
    "        # For disjointed subcatchments: pass all polygon outer rings\n",
    "        coordinates = geometry['coordinates']  # Already formatted as list of outer rings\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported geometry type: {geometry['type']}\")\n",
    "    \n",
    "    params = {\n",
    "        'dataset': 'OPENET_CONUS',\n",
    "        'variable': variable,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'coordinates': str(coordinates),\n",
    "        'area_reducer': area_reducer\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = api_session.get(url, headers=HEADERS, params=params, timeout=60)\n",
    "        if resp.ok:\n",
    "            raw_response = resp.json()\n",
    "            \n",
    "            # Count NoData in raw response BEFORE aggregation\n",
    "            variable_key = f\"{variable} (mm)\"\n",
    "            total_entries = 0\n",
    "            nodata_count = 0\n",
    "            \n",
    "            if 'Data' in raw_response:\n",
    "                for polygon_data in raw_response['Data']:\n",
    "                    if 'Data' in polygon_data:\n",
    "                        for entry in polygon_data['Data']:\n",
    "                            if variable_key in entry:\n",
    "                                total_entries += 1\n",
    "                                if entry[variable_key] == -9999.0:\n",
    "                                    nodata_count += 1\n",
    "            \n",
    "            # If multiple polygons, aggregate the results (filters NoData)\n",
    "            if geometry['type'] == 'MultiPolygon' and len(raw_response.get('Data', [])) > 1:\n",
    "                processed_response = aggregate_multipolygon_results(raw_response, variable)\n",
    "            else:\n",
    "                processed_response = raw_response\n",
    "            \n",
    "            return {\n",
    "                'data': processed_response,\n",
    "                'nodata_stats': {\n",
    "                    'total_entries': total_entries,\n",
    "                    'nodata_count': nodata_count\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            print(f'Error {resp.status_code}: {resp.text[:200]}')\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f'Exception: {e}')\n",
    "        return None\n",
    "\n",
    "def aggregate_multipolygon_results(api_response, variable):\n",
    "    \"\"\"\n",
    "    Aggregate results from multiple polygons into a single timeseries.\n",
    "    Takes the mean across all polygons for each date.\n",
    "    Filters out NoData values (-9999.0) before aggregation.\n",
    "    \"\"\"\n",
    "    if 'Data' not in api_response or len(api_response['Data']) <= 1:\n",
    "        return api_response\n",
    "    \n",
    "    # Collect all timeseries data from each polygon\n",
    "    all_polygon_data = []\n",
    "    for polygon_result in api_response['Data']:\n",
    "        if 'Data' in polygon_result:\n",
    "            all_polygon_data.append(polygon_result['Data'])\n",
    "    \n",
    "    if not all_polygon_data:\n",
    "        return api_response\n",
    "    \n",
    "    # Aggregate by date - take mean across all polygons\n",
    "    variable_key = f\"{variable} (mm)\"\n",
    "    date_values = {}\n",
    "    \n",
    "    for polygon_timeseries in all_polygon_data:\n",
    "        for entry in polygon_timeseries:\n",
    "            if 'Date' in entry and variable_key in entry:\n",
    "                date = entry['Date']\n",
    "                value = entry[variable_key]\n",
    "                # Filter out NoData values (-9999.0)\n",
    "                if value != -9999.0:\n",
    "                    if date not in date_values:\n",
    "                        date_values[date] = []\n",
    "                    date_values[date].append(value)\n",
    "    \n",
    "    # Create aggregated timeseries\n",
    "    aggregated_data = []\n",
    "    for date, values in sorted(date_values.items()):\n",
    "        if values:  # Only include dates with valid data\n",
    "            aggregated_data.append({\n",
    "                'Date': date,\n",
    "                variable_key: np.mean(values)  # Mean across all valid polygons\n",
    "            })\n",
    "    \n",
    "    # Return in same format as API response\n",
    "    return {\n",
    "        'Data': [{\n",
    "            'Data': aggregated_data\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# Test with first subcatchment\n",
    "test_subcatchment = unique_subcatchments.iloc[0]\n",
    "print(f\"Testing with TopazID: {test_subcatchment['topaz_id']}\")\n",
    "print(f\"Geometry type: {test_subcatchment['geometry']['type']}\")\n",
    "\n",
    "test_result = fetch_et_timeseries(\n",
    "    geometry=test_subcatchment['geometry'],\n",
    "    variable='et_ensemble_mad',\n",
    "    start_date='2020-01-01',\n",
    "    end_date='2020-12-31'\n",
    ")\n",
    "\n",
    "if test_result and test_result['data']:\n",
    "    print(f\"\\nAPI Response structure:\")\n",
    "    print(json.dumps(test_result['data'], indent=1)[:1000] + \"...\")\n",
    "    print(f\"\\nNoData Stats: {test_result['nodata_stats']}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Test request failed - check your network connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53891edd",
   "metadata": {},
   "source": [
    "## Step 4: Process All Subcatchments and Create Final Table\n",
    "\n",
    "Fetch ET data for all subcatchments and multiple models, then format into the requested table structure.\n",
    "\n",
    "**NoData Handling:** The API may return -9999.0 as a NoData indicator. These values are automatically filtered out during processing.\n",
    "\n",
    "**Important:** NoData tracking happens on the **raw API response** before any MultiPolygon aggregation. This ensures accurate counting even when:\n",
    "- All polygons in a MultiPolygon return NoData\n",
    "- Some polygons have valid data while others have NoData\n",
    "- The aggregation process filters out NoData values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b87634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 392 subcatchments\n",
      "Fetching 1 variables\n",
      "Date range: 2020-01-01 to 2023-12-31\n",
      "============================================================\n",
      "Processing all 392 subcatchments\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7d23f539ff474bac8e4e687ce0691c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/392 [00:00<?, ?subcatchment/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Total API requests: 392\n",
      "Successful: 392\n",
      "Failed: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "NoData Statistics:\n",
      "  Subcatchments with NoData: 179\n",
      "  Total NoData values filtered: 407\n",
      "  Overall NoData rate: 2.68%\n",
      "\n",
      "  Top 5 subcatchments by NoData rate:\n",
      "    - TopazID 1173: 7.3%\n",
      "    - TopazID 1392: 7.1%\n",
      "    - TopazID 1693: 6.8%\n",
      "    - TopazID 42: 6.2%\n",
      "    - TopazID 883: 6.2%\n",
      "\n",
      "Final Dataset Summary:\n",
      "  Total records: 18,635\n",
      "  Unique subcatchments: 392\n",
      "  Unique models: 1\n",
      "  Date range: 2020-2023\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topaz_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>model</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>4.09895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>17.74270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>37.27160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>78.53820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>108.26160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>106.68720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>121.75590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>116.93020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>73.34215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>29.85235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topaz_id  year  month            model      value\n",
       "0        32  2020      1  et_ensemble_mad    4.09895\n",
       "1        32  2020      2  et_ensemble_mad   17.74270\n",
       "2        32  2020      3  et_ensemble_mad   37.27160\n",
       "3        32  2020      4  et_ensemble_mad   78.53820\n",
       "4        32  2020      5  et_ensemble_mad  108.26160\n",
       "5        32  2020      6  et_ensemble_mad  106.68720\n",
       "6        32  2020      7  et_ensemble_mad  121.75590\n",
       "7        32  2020      8  et_ensemble_mad  116.93020\n",
       "8        32  2020      9  et_ensemble_mad   73.34215\n",
       "9        32  2020     10  et_ensemble_mad   29.85235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_timeseries_response(response_data, topaz_id, variable):\n",
    "    \"\"\"\n",
    "    Parse API response and extract year, month, value data.\n",
    "    Filters out NoData values (-9999.0) from the results.\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries with topaz_id, year, month, model, value\n",
    "    - NoData values are excluded from the output\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if not response_data or 'Data' not in response_data:\n",
    "        return results\n",
    "    \n",
    "    # The API returns data in a nested structure\n",
    "    # response_data['Data'] is a list with one element per coordinate/polygon\n",
    "    # Each element has a 'Data' field with the actual timeseries\n",
    "    data_list = response_data['Data']\n",
    "    \n",
    "    for coordinate_data in data_list:\n",
    "        if 'Data' not in coordinate_data:\n",
    "            continue\n",
    "            \n",
    "        # Extract the timeseries data\n",
    "        timeseries = coordinate_data['Data']\n",
    "        \n",
    "        # Create the variable key (e.g., 'et_ensemble_mad (mm)')\n",
    "        variable_key = f\"{variable} (mm)\"\n",
    "        \n",
    "        for entry in timeseries:\n",
    "            if 'Date' in entry and variable_key in entry:\n",
    "                # Parse date\n",
    "                date_str = entry['Date']\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                    year = date_obj.year\n",
    "                    month = date_obj.month\n",
    "                    value = entry[variable_key]\n",
    "                    \n",
    "                    # Filter out NoData values (-9999.0)\n",
    "                    if value == -9999.0:\n",
    "                        continue\n",
    "                    \n",
    "                    results.append({\n",
    "                        'topaz_id': topaz_id,\n",
    "                        'year': year,\n",
    "                        'month': month,\n",
    "                        'model': variable,\n",
    "                        'value': value\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing entry {entry}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Configuration\n",
    "START_YEAR = 2020\n",
    "END_YEAR = 2023\n",
    "START_DATE = f'{START_YEAR}-01-01'\n",
    "END_DATE = f'{END_YEAR}-12-31'\n",
    "\n",
    "# ET variables to fetch (you can add more from the metadata query)\n",
    "ET_VARIABLES = [\n",
    "    'et_ensemble_mad',\n",
    "    # 'et_disalexi',\n",
    "    # 'et_eemetric',\n",
    "    # 'et_geesebal',\n",
    "    # 'et_pt_jpl',\n",
    "    # 'et_sims',\n",
    "    # 'et_ssebop'\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(unique_subcatchments)} subcatchments\")\n",
    "print(f\"Fetching {len(ET_VARIABLES)} variables\")\n",
    "print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Collect all data\n",
    "all_data = []\n",
    "\n",
    "# Toggle between test mode and full processing\n",
    "TEST_MODE = False  # Set to True for testing, False to process all\n",
    "test_limit = 5     # Number of subcatchments to test with\n",
    "\n",
    "# Select subcatchments to process\n",
    "if TEST_MODE:\n",
    "    subcatchments_to_process = unique_subcatchments.head(test_limit)\n",
    "    total = test_limit\n",
    "    print(f\"⚠ TEST MODE: Processing only {test_limit} subcatchments\")\n",
    "else:\n",
    "    subcatchments_to_process = unique_subcatchments\n",
    "    total = len(unique_subcatchments)\n",
    "    print(f\"Processing all {total} subcatchments\")\n",
    "\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Stats tracking\n",
    "failed_requests = []\n",
    "successful_requests = 0\n",
    "total_requests = total * len(ET_VARIABLES)\n",
    "nodata_by_subcatchment = {}  # Track NoData occurrences\n",
    "\n",
    "# Main processing loop with progress bar\n",
    "for idx, row in tqdm(subcatchments_to_process.iterrows(), total=len(subcatchments_to_process), \n",
    "                      desc=\"Overall Progress\", unit=\"subcatchment\"):\n",
    "    topaz_id = row['topaz_id']\n",
    "    geometry = row['geometry']\n",
    "    \n",
    "    for variable in ET_VARIABLES:\n",
    "        # Fetch data (NoData tracking happens inside the function)\n",
    "        result = fetch_et_timeseries(\n",
    "            geometry=geometry,\n",
    "            variable=variable,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE\n",
    "        )\n",
    "        \n",
    "        if result and result['data']:\n",
    "            # Extract NoData statistics\n",
    "            nodata_count = result['nodata_stats']['nodata_count']\n",
    "            total_entries = result['nodata_stats']['total_entries']\n",
    "            \n",
    "            # Parse the processed response\n",
    "            parsed_data = process_timeseries_response(result['data'], topaz_id, variable)\n",
    "            \n",
    "            # Track NoData statistics\n",
    "            if nodata_count > 0:\n",
    "                if topaz_id not in nodata_by_subcatchment:\n",
    "                    nodata_by_subcatchment[topaz_id] = {'total': 0, 'nodata': 0}\n",
    "                nodata_by_subcatchment[topaz_id]['total'] += total_entries\n",
    "                nodata_by_subcatchment[topaz_id]['nodata'] += nodata_count\n",
    "            \n",
    "            all_data.extend(parsed_data)\n",
    "            successful_requests += 1\n",
    "        else:\n",
    "            failed_requests.append({'topaz_id': topaz_id, 'variable': variable})\n",
    "        \n",
    "        # Be nice to the API\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Create final DataFrame\n",
    "final_df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total API requests: {total_requests}\")\n",
    "print(f\"Successful: {successful_requests}\")\n",
    "print(f\"Failed: {len(failed_requests)}\")\n",
    "print(f\"Success rate: {100*successful_requests/total_requests:.1f}%\")\n",
    "\n",
    "if failed_requests:\n",
    "    print(f\"\\n⚠ Warning: {len(failed_requests)} requests failed:\")\n",
    "    for item in failed_requests[:5]:\n",
    "        print(f\"  - TopazID {item['topaz_id']}, variable {item['variable']}\")\n",
    "    if len(failed_requests) > 5:\n",
    "        print(f\"  ... and {len(failed_requests)-5} more\")\n",
    "\n",
    "if nodata_by_subcatchment:\n",
    "    total_nodata = sum(info['nodata'] for info in nodata_by_subcatchment.values())\n",
    "    total_values = sum(info['total'] for info in nodata_by_subcatchment.values())\n",
    "    nodata_rate = total_nodata / total_values * 100 if total_values > 0 else 0\n",
    "    \n",
    "    print(f\"\\nNoData Statistics:\")\n",
    "    print(f\"  Subcatchments with NoData: {len(nodata_by_subcatchment)}\")\n",
    "    print(f\"  Total NoData values filtered: {total_nodata:,}\")\n",
    "    print(f\"  Overall NoData rate: {nodata_rate:.2f}%\")\n",
    "    \n",
    "    # Show worst cases\n",
    "    worst_cases = sorted(\n",
    "        [(tid, info['nodata']/info['total']*100) for tid, info in nodata_by_subcatchment.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    if worst_cases:\n",
    "        print(f\"\\n  Top 5 subcatchments by NoData rate:\")\n",
    "        for tid, rate in worst_cases:\n",
    "            print(f\"    - TopazID {tid}: {rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nFinal Dataset Summary:\")\n",
    "print(f\"  Total records: {len(final_df):,}\")\n",
    "print(f\"  Unique subcatchments: {final_df['topaz_id'].nunique()}\")\n",
    "print(f\"  Unique models: {final_df['model'].nunique()}\")\n",
    "print(f\"  Date range: {final_df['year'].min()}-{final_df['year'].max()}\")\n",
    "\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "display(final_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f893866",
   "metadata": {},
   "source": [
    "## Step 5: Save and Analyze Results\n",
    "\n",
    "Display summary statistics and save the final table to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9515a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Table Structure:\n",
      "Columns: ['topaz_id', 'year', 'month', 'model', 'value']\n",
      "\n",
      "Sample data (topaz_id, year, month, model, value):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topaz_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>model</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>4.09895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>17.74270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>37.27160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>78.53820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>108.26160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>106.68720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>121.75590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>116.93020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>73.34215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>29.85235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>9.31655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>14.71270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>9.82045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>16.01580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>39.82195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>89.94115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>113.34565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>138.01845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>141.56895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>et_ensemble_mad</td>\n",
       "      <td>116.01695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topaz_id  year  month            model      value\n",
       "0         32  2020      1  et_ensemble_mad    4.09895\n",
       "1         32  2020      2  et_ensemble_mad   17.74270\n",
       "2         32  2020      3  et_ensemble_mad   37.27160\n",
       "3         32  2020      4  et_ensemble_mad   78.53820\n",
       "4         32  2020      5  et_ensemble_mad  108.26160\n",
       "5         32  2020      6  et_ensemble_mad  106.68720\n",
       "6         32  2020      7  et_ensemble_mad  121.75590\n",
       "7         32  2020      8  et_ensemble_mad  116.93020\n",
       "8         32  2020      9  et_ensemble_mad   73.34215\n",
       "9         32  2020     10  et_ensemble_mad   29.85235\n",
       "10        32  2020     11  et_ensemble_mad    9.31655\n",
       "11        32  2020     12  et_ensemble_mad   14.71270\n",
       "12        32  2021      1  et_ensemble_mad    9.82045\n",
       "13        32  2021      2  et_ensemble_mad   16.01580\n",
       "14        32  2021      3  et_ensemble_mad   39.82195\n",
       "15        32  2021      4  et_ensemble_mad   89.94115\n",
       "16        32  2021      5  et_ensemble_mad  113.34565\n",
       "17        32  2021      6  et_ensemble_mad  138.01845\n",
       "18        32  2021      7  et_ensemble_mad  141.56895\n",
       "19        32  2021      8  et_ensemble_mad  116.01695"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary by Model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>et_ensemble_mad</th>\n",
       "      <td>18635</td>\n",
       "      <td>58.523257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>46.315797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count       mean  min    max        std\n",
       "model                                                   \n",
       "et_ensemble_mad  18635  58.523257  0.0  167.0  46.315797"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary by Year:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>4704</td>\n",
       "      <td>56.422447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>4646</td>\n",
       "      <td>61.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>4704</td>\n",
       "      <td>52.459184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>4581</td>\n",
       "      <td>63.962523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count       mean\n",
       "year                  \n",
       "2020   4704  56.422447\n",
       "2021   4646  61.426900\n",
       "2022   4704  52.459184\n",
       "2023   4581  63.962523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA SAVED SUCCESSFULLY\n",
      "============================================================\n",
      "Data file: data/et_data_by_subcatchment.parquet\n",
      "Metadata file: data/et_data_by_subcatchment_metadata.json\n",
      "Total records: 18,635\n",
      "File size: 0.13 MB\n",
      "\n",
      "To read the data later:\n",
      "  df = pd.read_parquet('data/et_data_by_subcatchment.parquet')\n"
     ]
    }
   ],
   "source": [
    "# Display sample of final table with all columns\n",
    "print(\"Final Table Structure:\")\n",
    "print(f\"Columns: {final_df.columns.tolist()}\")\n",
    "print(f\"\\nSample data (topaz_id, year, month, model, value):\")\n",
    "display(final_df[['topaz_id', 'year', 'month', 'model', 'value']].head(20))\n",
    "\n",
    "# Summary statistics by model\n",
    "print(\"\\n\\nSummary by Model:\")\n",
    "summary = final_df.groupby('model')['value'].agg(['count', 'mean', 'min', 'max', 'std'])\n",
    "display(summary)\n",
    "\n",
    "# Summary by year\n",
    "print(\"\\n\\nSummary by Year:\")\n",
    "yearly = final_df.groupby('year')['value'].agg(['count', 'mean'])\n",
    "display(yearly)\n",
    "\n",
    "# Save to data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save to Parquet (more efficient than CSV)\n",
    "output_file = 'data/et_data_by_subcatchment.parquet'\n",
    "final_df.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Also save metadata\n",
    "import json\n",
    "\n",
    "# Calculate NoData statistics for metadata\n",
    "nodata_stats = {}\n",
    "if nodata_by_subcatchment:\n",
    "    total_nodata = sum(info['nodata'] for info in nodata_by_subcatchment.values())\n",
    "    total_values = sum(info['total'] for info in nodata_by_subcatchment.values())\n",
    "    nodata_stats = {\n",
    "        'subcatchments_with_nodata': len(nodata_by_subcatchment),\n",
    "        'total_nodata_filtered': total_nodata,\n",
    "        'total_values_examined': total_values,\n",
    "        'nodata_rate_percent': round(total_nodata / total_values * 100, 2) if total_values > 0 else 0\n",
    "    }\n",
    "\n",
    "metadata = {\n",
    "    'date_created': pd.Timestamp.now().isoformat(),\n",
    "    'num_records': len(final_df),\n",
    "    'num_subcatchments': final_df['topaz_id'].nunique(),\n",
    "    'models': final_df['model'].unique().tolist(),\n",
    "    'date_range': {\n",
    "        'start': START_DATE,\n",
    "        'end': END_DATE\n",
    "    },\n",
    "    'years': sorted(final_df['year'].unique().tolist()),\n",
    "    'data_quality': {\n",
    "        'nodata_filtering': 'Enabled - values of -9999.0 excluded',\n",
    "        **nodata_stats\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = 'data/et_data_by_subcatchment_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATA SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Data file: {output_file}\")\n",
    "print(f\"Metadata file: {metadata_file}\")\n",
    "print(f\"Total records: {len(final_df):,}\")\n",
    "\n",
    "# Show file size\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTo read the data later:\")\n",
    "print(f\"  df = pd.read_parquet('{output_file}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c7bd9",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Handling NoData Values:\n",
    "The Climate Engine API may return **-9999.0** as a NoData indicator for certain dates/locations. The script automatically handles this:\n",
    "- **Filtering**: All -9999.0 values are excluded from the output\n",
    "- **Aggregation**: For MultiPolygon subcatchments, NoData values are filtered before computing means\n",
    "- **Tracking**: The script tracks and reports how many NoData values were encountered\n",
    "- **Metadata**: NoData statistics are saved to the metadata file for quality assessment\n",
    "\n",
    "**Impact**: If a subcatchment has high NoData rates, it may indicate:\n",
    "- The location is outside the OpenET coverage area\n",
    "- Data is not available for the requested time period\n",
    "- The geometry may need verification\n",
    "\n",
    "To diagnose NoData issues in detail, run: `python diagnose_nodata.py`\n",
    "\n",
    "### Handling Disjointed Subcatchments:\n",
    "The script automatically detects and handles subcatchments that consist of multiple disjointed polygons (same TopazID, multiple features). According to the Climate Engine API documentation:\n",
    "- The API accepts multiple polygons in a single request as: `[[polygon1_coords], [polygon2_coords], ...]`\n",
    "- The API returns **separate** data for each polygon\n",
    "- The script aggregates results by computing the **mean** across all polygons for each date\n",
    "- This approach assumes all polygon parts of a subcatchment should be weighted equally\n",
    "\n",
    "### Handling Polygons with Holes (Interior Rings):\n",
    "According to the GeoJSON specification (RFC 7946), polygons can have interior rings (holes):\n",
    "- `coordinates[0]` = exterior ring (outer boundary)\n",
    "- `coordinates[1], coordinates[2], ...` = interior rings (holes)\n",
    "\n",
    "**Current Implementation:**\n",
    "- The script currently uses **only the outer ring** of each polygon\n",
    "- Interior rings (holes) are **ignored**\n",
    "- This may cause **slight overestimation** of ET values for polygons with holes (calculating ET over the hole areas)\n",
    "\n",
    "The Climate Engine API documentation doesn't clearly specify support for polygon holes, and testing (Step 1) helps determine if the API properly handles them. If your data has polygons with holes and accurate ET calculation is critical, you may need to contact Climate Engine support for clarification on hole handling\n",
    "\n",
    "### To process all subcatchments:\n",
    "1. In Step 4, change `TEST_MODE = False` to `True`\n",
    "2. Or modify the subcatchment selection logic as needed\n",
    "3. Processing all subcatchments may take considerable time depending on the number\n",
    "\n",
    "### To add more ET models:\n",
    "Uncomment additional variables in the `ET_VARIABLES` list in Step 4. Available variables from OpenET include:\n",
    "- `et_ensemble_mad` - Ensemble median absolute deviation\n",
    "- `et_disalexi` - DisALEXI model\n",
    "- `et_eemetric` - eeMETRIC model\n",
    "- `et_geesebal` - geeSEBAL model  \n",
    "- `et_pt_jpl` - PT-JPL model\n",
    "- `et_sims` - SIMS model\n",
    "- `et_ssebop` - SSEBop model\n",
    "\n",
    "### API Rate Limiting:\n",
    "The code includes a 0.5 second delay between API calls. Adjust the `time.sleep()` value if you encounter rate limiting errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
