{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4f565f",
   "metadata": {},
   "source": [
    "# OpenET Data Scraper\n",
    "Scrapes OpenET data from the ClimateEngine API for all input subcatchments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f805ad",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b63131",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "To run this notebook, Python and some python dependencies are required. For convenience, a `requirements.txt` will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1b7db",
   "metadata": {},
   "source": [
    "#### API Key\n",
    "Define an environment variable `CLIMATE_ENGINE_API_KEY` with your climate engine API key. If you do not have a key, [request a key](https://support.climateengine.org/article/36-requesting-an-authorization-key-token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5a4da3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057fe53",
   "metadata": {},
   "source": [
    "#### Subcatchments Manifest\n",
    "Provide a manifest for the locations of all the relevant subcatchments and their geometries. This script currently supports `.yaml` manifests with the following structure:\n",
    "```\n",
    "Subcatchments:\n",
    "  - name:\n",
    "    url:\n",
    "  - name:\n",
    "    url:\n",
    "  ...\n",
    "```\n",
    "where the urls points to geojson with subcatchments as features. From each data file, the topaz id and geometry will be used to create the final output.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ebc23869",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBCATCHMENTS_MANIFEST = 'subcatchment-manifest.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e83997",
   "metadata": {},
   "source": [
    "#### Misc. Configuration\n",
    "Some configuration options will be available below for tweaking/debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fba57e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True # Detailed logging\n",
    "API_BASE_URL = 'https://api.climateengine.org'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877e182",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "The functions used will be defined below in a way that explains the implementation for building the output of one watershed. For a simpler view of the entire process, see the [main loop](#main-loop)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621fb28",
   "metadata": {},
   "source": [
    "#### Retry Logic\n",
    "\n",
    "The requests made in this project occasionally fail, but can be fixed with a simple retry. Thus, requests will be made with retry-enabled aiohttp requests. The aiohttp library is used over the requests library for compatibility with asyncio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4ff4b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    "    retry_if_result\n",
    ")\n",
    "\n",
    "def is_server_error(response):\n",
    "    \"\"\"Check if response status indicates a server error.\"\"\"\n",
    "    return response is not None and response.status >= 500\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=10),\n",
    "    retry=(\n",
    "        retry_if_exception_type((aiohttp.ClientError, TimeoutError)) |\n",
    "        retry_if_result(is_server_error)\n",
    "    ),\n",
    "    reraise=True  # Re-raise so we can catch and handle at call site\n",
    ")\n",
    "async def fetch_with_retry(session: aiohttp.ClientSession, method: str, url: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Fetch a URL with automatic retry logic using tenacity.\n",
    "    \n",
    "    Parameters:\n",
    "    - session: aiohttp ClientSession\n",
    "    - method: HTTP method ('GET', 'POST', etc.)\n",
    "    - url: URL to fetch\n",
    "    - **kwargs: Additional arguments to pass to session.request()\n",
    "    \n",
    "    Returns:\n",
    "    - aiohttp.ClientResponse object\n",
    "    \n",
    "    Raises:\n",
    "    - Exception if all retries are exhausted\n",
    "    \n",
    "    Retries on:\n",
    "    - Server errors (5xx status codes)\n",
    "    - Network errors (ClientError, TimeoutError)\n",
    "    - Uses exponential backoff (1s, 2s, 4s, 8s, max 10s)\n",
    "    \"\"\"\n",
    "    response = await session.request(method, url, **kwargs)\n",
    "    \n",
    "    # If server error, trigger retry\n",
    "    if response.status >= 500:\n",
    "        await response.release()\n",
    "        return response  # Will trigger retry via retry_if_result\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2a12b",
   "metadata": {},
   "source": [
    "#### Prepare Subcatchments\n",
    "The subcatchment topaz ids and geometries need to be collected - this function fetches them for a single watershed and returns them in a Pandas dataframe. Note that multiple geometries can exist for the same topazid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c7e73f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "async def get_subcatchments(geojson_url: str, verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches the geojson data from a URL and returns a pandas DataFrame of subcatchment geometries.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            response = await fetch_with_retry(session, 'GET', geojson_url)\n",
    "            \n",
    "            if response.status == 200:\n",
    "                geojson_data = await response.json()\n",
    "                logger.info(f\"Loaded {len(geojson_data['features'])} subcatchment features (geometries)\")\n",
    "                \n",
    "                # Extract unique TopazIDs and their geometries\n",
    "                subcatchments = []\n",
    "                for feature in geojson_data['features']:\n",
    "                    topaz_id = feature['properties']['TopazID']\n",
    "                    geometry = feature['geometry']\n",
    "                    subcatchments.append({'topaz_id': topaz_id, 'geometry': geometry})\n",
    "                \n",
    "                # Create DataFrame and get unique subcatchments\n",
    "                subcatchments_df = pd.DataFrame(subcatchments)\n",
    "                unique_subcatchments = subcatchments_df.groupby('topaz_id').first().reset_index()\n",
    "                logger.info(f\"Found {len(unique_subcatchments)} unique TopazIDs\")\n",
    "                logger.info(f\"Sample TopazIDs: {unique_subcatchments['topaz_id'].head().tolist()}\")\n",
    "\n",
    "                return subcatchments_df\n",
    "            else:\n",
    "                text = await response.text()\n",
    "                logger.error(f'Error {response.status}: {text}')\n",
    "                return pd.DataFrame()\n",
    "        except aiohttp.ClientError as e:\n",
    "            logger.error(f'Network error fetching subcatchments (after retries): {type(e).__name__}: {str(e)}')\n",
    "            return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "\n",
    "            logger.error(f'Failed to fetch subcatchments (after retries): {type(e).__name__}: {str(e)}')\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240bd4d",
   "metadata": {},
   "source": [
    "#### Fetch OpenET Timeseries\n",
    "\n",
    "The ClimateEngine API accepts geometries and can return ET timeseries data. For more information, see:\n",
    "* https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-coordinates\n",
    "* https://support.climateengine.org/article/152-formatting-coordinates-for-api-requests\n",
    "\n",
    "The following functions concurrently fetch ET timeseries data from the API for each subcatchment in a watershed. The concurrency is implemented using asyncio. \n",
    "\n",
    "Single request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a5d84433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "async def fetch_single_et_timeseries(\n",
    "    session: aiohttp.ClientSession,\n",
    "    geometry: dict,\n",
    "    dataset: str,\n",
    "    variable: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    area_reducer: str = 'mean'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Fetch ET timeseries data for a polygon geometry.\n",
    "    \n",
    "    Parameters:\n",
    "    - session: aiohttp ClientSession (should be reused across multiple calls)\n",
    "    - geometry: GeoJSON geometry object (Polygon)\n",
    "    - dataset: Dataset name (e.g., 'OpenET_ENSEMBLE')\n",
    "    - variable: ET variable name (e.g., 'et_ensemble_mad')\n",
    "    - start_date: Start date (YYYY-MM-DD format)\n",
    "    - end_date: End date (YYYY-MM-DD format)\n",
    "    - area_reducer: Spatial aggregation method (default: 'mean')\n",
    "    \n",
    "    Returns:\n",
    "    - API response with timeseries data, or None if request fails\n",
    "    \n",
    "    Note: \n",
    "        - NoData values (-9999.0) are NOT filtered here\n",
    "        - Uses automatic retry with exponential backoff via tenacity.\n",
    "        - Coordinates must be formatted as per ClimateEngine API spec:\n",
    "          For polygons: [[[-121.61,38.78],[-121.52,38.78],[-121.52,38.83],[-121.61,38.83],[-121.61,38.78]]]\n",
    "    \"\"\"\n",
    "    url = f'{API_BASE_URL}/timeseries/native/coordinates'\n",
    "    \n",
    "    # Prepare query parameters with coordinates as JSON string\n",
    "    params = {\n",
    "        'dataset': dataset,\n",
    "        'variable': variable,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'area_reducer': area_reducer,\n",
    "        'coordinates': json.dumps(geometry['coordinates'])\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = await fetch_with_retry(session, 'GET', url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            return data\n",
    "        else:\n",
    "            text = await response.text()\n",
    "            logger.error(f'ET API request failed with status {response.status}: {text[:200]}')\n",
    "            return None\n",
    "            \n",
    "    except aiohttp.ClientError as e:\n",
    "        logger.error(f'Network error during ET API request (after retries): {type(e).__name__}: {str(e)}')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f'Exception during ET API request (after retries): {type(e).__name__}: {str(e)}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206d924",
   "metadata": {},
   "source": [
    "Requests for entire watershed (concurrent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a4f6729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def fetch_all_et_data(\n",
    "    subcatchments_df: pd.DataFrame,\n",
    "    dataset: str = 'OpenET_ENSEMBLE',\n",
    "    variable: str = 'et_ensemble_mad',\n",
    "    start_date: str = '2023-01-01',\n",
    "    end_date: str = '2023-12-31',\n",
    "    area_reducer: str = 'mean',\n",
    "    max_concurrent: int = 1000\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Fetch ET timeseries data concurrently for all subcatchments with progress bar.\n",
    "    \n",
    "    Parameters:\n",
    "    - subcatchments_df: DataFrame with 'topaz_id' and 'geometry' columns\n",
    "    - dataset: Dataset name (default: 'OpenET_ENSEMBLE')\n",
    "    - variable: ET variable name (default: 'et_ensemble_mad')\n",
    "    - start_date: Start date (YYYY-MM-DD)\n",
    "    - end_date: End date (YYYY-MM-DD)\n",
    "    - area_reducer: Spatial aggregation method (default: 'mean')\n",
    "    - max_concurrent: Maximum concurrent requests (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (DataFrame with topaz_id, date, and ET value columns, fetch statistics dict)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Track success/failure statistics\n",
    "    successful_fetches = 0\n",
    "    failed_fetches = 0\n",
    "    failed_topaz_ids = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Use asyncio.Semaphore to limit concurrent requests\n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        # Create async wrapper for each fetch with semaphore\n",
    "        async def fetch_with_semaphore(topaz_id, geometry):\n",
    "            async with semaphore:\n",
    "                response = await fetch_single_et_timeseries(\n",
    "                    session=session,\n",
    "                    geometry=geometry,\n",
    "                    dataset=dataset,\n",
    "                    variable=variable,\n",
    "                    start_date=start_date,\n",
    "                    end_date=end_date,\n",
    "                    area_reducer=area_reducer\n",
    "                )\n",
    "                return topaz_id, response\n",
    "        \n",
    "        # Create task objects (not coroutines) for all subcatchments\n",
    "        tasks = [\n",
    "            asyncio.create_task(fetch_with_semaphore(row['topaz_id'], row['geometry']))\n",
    "            for _, row in subcatchments_df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting concurrent fetch for {len(tasks)} subcatchments...\")\n",
    "        print(f\"Max concurrent requests: {max_concurrent}\")\n",
    "        \n",
    "        # Use tqdm.asyncio.gather for progress bar with actual task objects\n",
    "        responses = []\n",
    "        for coro in tqdm.as_completed(tasks, total=len(tasks), desc=\"Fetching ET data\"):\n",
    "            result = await coro\n",
    "            responses.append(result)\n",
    "        \n",
    "        # Process results\n",
    "        for topaz_id, response in responses:\n",
    "            if isinstance(response, Exception):\n",
    "                logger.error(f\"Failed to fetch data for TopazID {topaz_id}: {str(response)}\")\n",
    "                failed_fetches += 1\n",
    "                failed_topaz_ids.append(topaz_id)\n",
    "                continue\n",
    "            \n",
    "            if response and \"Data\" in response:\n",
    "                successful_fetches += 1\n",
    "                for entry in response[\"Data\"][0][\"Data\"]:\n",
    "                    date = entry.get(\"Date\")\n",
    "                    value = entry.get(f\"{variable} (mm)\")\n",
    "                    results.append({\n",
    "                        \"topaz_id\": topaz_id,\n",
    "                        \"date\": date,\n",
    "                        \"et_value\": value\n",
    "                    })\n",
    "            else:\n",
    "                failed_fetches += 1\n",
    "                failed_topaz_ids.append(topaz_id)\n",
    "                if response is None:\n",
    "                    logger.warning(f\"No response returned for TopazID {topaz_id}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No data in response for TopazID {topaz_id}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_requests = len(tasks)\n",
    "    success_rate = (successful_fetches / total_requests * 100) if total_requests > 0 else 0\n",
    "    \n",
    "    print(f\"\\nCompleted in {elapsed_time:.2f} seconds ({elapsed_time/len(tasks):.2f}s per subcatchment)\")\n",
    "    print(f\"Success: {successful_fetches}/{total_requests} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    if failed_fetches > 0:\n",
    "        print(f\"⚠ Failed: {failed_fetches} subcatchments\")\n",
    "        if len(failed_topaz_ids) <= 10:\n",
    "            print(f\"  Failed TopazIDs: {failed_topaz_ids}\")\n",
    "        else:\n",
    "            print(f\"  Failed TopazIDs: {failed_topaz_ids[:10]} ... and {len(failed_topaz_ids)-10} more\")\n",
    "    \n",
    "    # Compile statistics - convert to native Python types for JSON serialization\n",
    "    fetch_stats = {\n",
    "        'total': int(total_requests),\n",
    "        'successful': int(successful_fetches),\n",
    "        'failed': int(failed_fetches),\n",
    "        'success_rate': float(success_rate),\n",
    "        'elapsed_time_seconds': float(elapsed_time),\n",
    "        'failed_topaz_ids': failed_topaz_ids\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(results), fetch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc992c7",
   "metadata": {},
   "source": [
    "#### Handling Multipolygons\n",
    "\n",
    "Subcatchments can take the form of a multipolygon (multiple disjointed polygons). This will manifest as multiple geometries associated with the same topazid in the table returned by the last step. There are several ways to handle this:\n",
    "* weighted average\n",
    "* drop other polygons and keep the largest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d2451",
   "metadata": {},
   "source": [
    "#### Data Quality: Missing Values\n",
    "\n",
    "The ClimateEngine API may return `-9999` (or `-9999.0`) to indicate missing data. For data integrity, we need to identify and filter out subcatchments that contain any missing values.\n",
    "\n",
    "**Approach:**\n",
    "- Identify all subcatchments (topaz_ids) that have at least one `-9999` value\n",
    "- Filter out these subcatchments completely from the dataset\n",
    "- This is done regardless of whether the subcatchment is a multipolygon with some valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "76c63903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_subcatchments_with_missing_values(et_data_df: pd.DataFrame, missing_value: float = -9999.0) -> set:\n",
    "    \"\"\"\n",
    "    Identify subcatchments that have any missing values in their ET data.\n",
    "    \n",
    "    Parameters:\n",
    "    - et_data_df: DataFrame with 'topaz_id' and 'et_value' columns\n",
    "    - missing_value: The value that indicates missing data (default: -9999.0)\n",
    "    \n",
    "    Returns:\n",
    "    - Set of topaz_ids that contain at least one missing value\n",
    "    \"\"\"\n",
    "    # Find all rows with missing values\n",
    "    missing_mask = et_data_df['et_value'] == missing_value\n",
    "    \n",
    "    # Get unique topaz_ids that have missing values\n",
    "    topaz_ids_with_missing = set(et_data_df[missing_mask]['topaz_id'].unique())\n",
    "    \n",
    "    return topaz_ids_with_missing\n",
    "\n",
    "\n",
    "def filter_out_missing_values(et_data_df: pd.DataFrame, subcatchments_df: pd.DataFrame, missing_value: float = -9999.0) -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Filter out subcatchments that have any missing values from both the ET data and subcatchments DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - et_data_df: DataFrame with 'topaz_id' and 'et_value' columns\n",
    "    - subcatchments_df: DataFrame with 'topaz_id' and 'geometry' columns\n",
    "    - missing_value: The value that indicates missing data (default: -9999.0)\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (filtered_et_data_df, filtered_subcatchments_df, filter_stats)\n",
    "    \"\"\"\n",
    "    # Identify subcatchments with missing values\n",
    "    topaz_ids_with_missing = identify_subcatchments_with_missing_values(et_data_df, missing_value)\n",
    "    \n",
    "    # Count statistics\n",
    "    total_subcatchments = et_data_df['topaz_id'].nunique()\n",
    "    num_with_missing = len(topaz_ids_with_missing)\n",
    "    num_retained = total_subcatchments - num_with_missing\n",
    "    \n",
    "    # Filter both dataframes\n",
    "    filtered_et_data = et_data_df[~et_data_df['topaz_id'].isin(topaz_ids_with_missing)].copy()\n",
    "    filtered_subcatchments = subcatchments_df[~subcatchments_df['topaz_id'].isin(topaz_ids_with_missing)].copy()\n",
    "    \n",
    "    # Calculate record statistics\n",
    "    total_records = len(et_data_df)\n",
    "    records_removed = len(et_data_df) - len(filtered_et_data)\n",
    "    \n",
    "    filter_stats = {\n",
    "        'total_subcatchments': int(total_subcatchments),\n",
    "        'subcatchments_with_missing': int(num_with_missing),\n",
    "        'subcatchments_retained': int(num_retained),\n",
    "        'total_records': int(total_records),\n",
    "        'records_removed': int(records_removed),\n",
    "        'records_retained': int(len(filtered_et_data))\n",
    "    }\n",
    "    \n",
    "    return filtered_et_data, filtered_subcatchments, filter_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de415af",
   "metadata": {},
   "source": [
    "#### Multipolygon Aggregation\n",
    "\n",
    "Some subcatchments consist of multiple disjointed polygons (multipolygons) represented by multiple geometries for the same topaz_id. We need to aggregate the ET data from these multiple polygons into a single value per time step.\n",
    "\n",
    "**Two aggregation strategies:**\n",
    "\n",
    "1. **Area-weighted aggregation**: Calculate a weighted average based on the area of each polygon\n",
    "   - More accurate representation of the entire subcatchment\n",
    "   - Preserves information from all polygons\n",
    "   - Requires calculating polygon areas\n",
    "\n",
    "2. **Largest polygon only**: Use the ET value from the largest polygon\n",
    "   - Simpler and faster\n",
    "   - May lose information from smaller polygons\n",
    "   - Assumes the largest polygon is most representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c57b9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "\n",
    "\n",
    "def calculate_polygon_area(geometry: dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the area of a polygon in square meters.\n",
    "    \n",
    "    Parameters:\n",
    "    - geometry: GeoJSON geometry object\n",
    "    \n",
    "    Returns:\n",
    "    - Area in square meters\n",
    "    \"\"\"\n",
    "    # Convert GeoJSON to shapely geometry\n",
    "    geom = shape(geometry)\n",
    "    \n",
    "    # Get the centroid to determine appropriate UTM zone\n",
    "    centroid = geom.centroid\n",
    "    lon, lat = centroid.x, centroid.y\n",
    "    \n",
    "    # Calculate UTM zone\n",
    "    utm_zone = int((lon + 180) / 6) + 1\n",
    "    \n",
    "    # Create transformer from WGS84 to UTM using modern API\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    utm = pyproj.CRS(f\"+proj=utm +zone={utm_zone} +datum=WGS84 +units=m +no_defs\")\n",
    "    transformer = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True)\n",
    "    \n",
    "    # Transform and calculate area\n",
    "    geom_utm = transform(transformer.transform, geom)\n",
    "    return geom_utm.area\n",
    "\n",
    "\n",
    "def identify_multipolygons(subcatchments_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Identify which topaz_ids have multiple geometries (multipolygons).\n",
    "    \n",
    "    Parameters:\n",
    "    - subcatchments_df: DataFrame with 'topaz_id' and 'geometry' columns\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with statistics about multipolygons\n",
    "    \"\"\"\n",
    "    # Count geometries per topaz_id\n",
    "    geometry_counts = subcatchments_df.groupby('topaz_id').size()\n",
    "    \n",
    "    # Identify multipolygons (more than 1 geometry)\n",
    "    multipolygon_ids = geometry_counts[geometry_counts > 1].index.tolist()\n",
    "    \n",
    "    stats = {\n",
    "        'total_unique_topaz_ids': int(subcatchments_df['topaz_id'].nunique()),\n",
    "        'multipolygon_topaz_ids': int(len(multipolygon_ids)),\n",
    "        'single_polygon_topaz_ids': int(len(geometry_counts[geometry_counts == 1])),\n",
    "        'max_geometries_per_topaz_id': int(geometry_counts.max()),\n",
    "        'multipolygon_ids': multipolygon_ids\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def aggregate_multipolygons_weighted(\n",
    "    et_data_df: pd.DataFrame,\n",
    "    subcatchments_df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Aggregate multipolygon ET data using area-weighted averaging.\n",
    "    \n",
    "    For each topaz_id with multiple geometries:\n",
    "    1. Calculate the area of each polygon\n",
    "    2. For each time step, compute weighted average: sum(value_i * area_i) / sum(area_i)\n",
    "    \n",
    "    Parameters:\n",
    "    - et_data_df: DataFrame with 'topaz_id', 'date', and 'et_value' columns\n",
    "    - subcatchments_df: DataFrame with 'topaz_id' and 'geometry' columns\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (aggregated_et_data_df, aggregation_stats)\n",
    "    \"\"\"\n",
    "    # Calculate areas for all geometries\n",
    "    print(\"Calculating polygon areas...\")\n",
    "    subcatchments_df = subcatchments_df.copy()\n",
    "    subcatchments_df['area'] = subcatchments_df['geometry'].apply(calculate_polygon_area)\n",
    "    \n",
    "    # Create a mapping of (topaz_id, geometry_index) to area\n",
    "    # We need to track which geometry each ET value corresponds to\n",
    "    subcatchments_df['geometry_index'] = subcatchments_df.groupby('topaz_id').cumcount()\n",
    "    \n",
    "    # Merge ET data with geometry areas\n",
    "    # First, we need to add geometry_index to et_data\n",
    "    # For this, we'll merge with subcatchments on topaz_id and use geometry_index order\n",
    "    \n",
    "    # Create geometry_index in et_data based on the order from subcatchments\n",
    "    et_data_with_geometry = et_data_df.copy()\n",
    "    \n",
    "    # For each topaz_id, assign geometry_index based on order in subcatchments_df\n",
    "    geometry_lookup = subcatchments_df.set_index(['topaz_id', 'geometry_index'])['area'].to_dict()\n",
    "    \n",
    "    # Create a geometry_index for ET data by counting occurrences\n",
    "    et_data_with_geometry['geometry_index'] = et_data_with_geometry.groupby('topaz_id').cumcount() % subcatchments_df.groupby('topaz_id').size().max()\n",
    "    \n",
    "    # Map areas to ET data\n",
    "    et_data_with_geometry['area'] = et_data_with_geometry.apply(\n",
    "        lambda row: geometry_lookup.get((row['topaz_id'], row['geometry_index']), 1.0),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate weighted values\n",
    "    et_data_with_geometry['weighted_value'] = et_data_with_geometry['et_value'] * et_data_with_geometry['area']\n",
    "    \n",
    "    # Group by topaz_id and date, then calculate weighted average\n",
    "    aggregated = et_data_with_geometry.groupby(['topaz_id', 'date']).agg({\n",
    "        'weighted_value': 'sum',\n",
    "        'area': 'sum',\n",
    "        'et_value': 'count'  # Count number of polygons\n",
    "    }).reset_index()\n",
    "    \n",
    "    aggregated['et_value'] = aggregated['weighted_value'] / aggregated['area']\n",
    "    aggregated = aggregated.rename(columns={'et_value_count': 'num_polygons'})\n",
    "    aggregated = aggregated[['topaz_id', 'date', 'et_value']]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    original_records = len(et_data_df)\n",
    "    aggregated_records = len(aggregated)\n",
    "    multipolygon_count = (et_data_df.groupby('topaz_id').size() > 1).sum()\n",
    "    \n",
    "    stats = {\n",
    "        'method': 'area_weighted',\n",
    "        'original_records': int(original_records),\n",
    "        'aggregated_records': int(aggregated_records),\n",
    "        'records_reduced': int(original_records - aggregated_records),\n",
    "        'multipolygon_topaz_ids': int(multipolygon_count)\n",
    "    }\n",
    "    \n",
    "    return aggregated, stats\n",
    "\n",
    "\n",
    "def aggregate_multipolygons_largest(\n",
    "    et_data_df: pd.DataFrame,\n",
    "    subcatchments_df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Aggregate multipolygon ET data by selecting only the largest polygon.\n",
    "    \n",
    "    For each topaz_id with multiple geometries:\n",
    "    1. Calculate the area of each polygon\n",
    "    2. Keep only the ET data from the largest polygon\n",
    "    \n",
    "    Parameters:\n",
    "    - et_data_df: DataFrame with 'topaz_id', 'date', and 'et_value' columns\n",
    "    - subcatchments_df: DataFrame with 'topaz_id' and 'geometry' columns\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (filtered_et_data_df, aggregation_stats)\n",
    "    \"\"\"\n",
    "    # Calculate areas for all geometries\n",
    "    print(\"Calculating polygon areas...\")\n",
    "    subcatchments_df = subcatchments_df.copy()\n",
    "    subcatchments_df['area'] = subcatchments_df['geometry'].apply(calculate_polygon_area)\n",
    "    \n",
    "    # For each topaz_id, find the index of the largest polygon\n",
    "    subcatchments_df['geometry_index'] = subcatchments_df.groupby('topaz_id').cumcount()\n",
    "    \n",
    "    # Find the geometry_index with max area for each topaz_id\n",
    "    largest_geometries = subcatchments_df.loc[\n",
    "        subcatchments_df.groupby('topaz_id')['area'].idxmax()\n",
    "    ][['topaz_id', 'geometry_index']].set_index('topaz_id')['geometry_index'].to_dict()\n",
    "    \n",
    "    # Add geometry_index to ET data\n",
    "    et_data_with_geometry = et_data_df.copy()\n",
    "    et_data_with_geometry['geometry_index'] = et_data_with_geometry.groupby('topaz_id').cumcount() % subcatchments_df.groupby('topaz_id').size().max()\n",
    "    \n",
    "    # Filter to keep only data from largest polygons\n",
    "    et_data_with_geometry['is_largest'] = et_data_with_geometry.apply(\n",
    "        lambda row: row['geometry_index'] == largest_geometries.get(row['topaz_id'], 0),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    filtered_data = et_data_with_geometry[et_data_with_geometry['is_largest']][['topaz_id', 'date', 'et_value']]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    original_records = len(et_data_df)\n",
    "    filtered_records = len(filtered_data)\n",
    "    multipolygon_count = len(largest_geometries)\n",
    "    \n",
    "    stats = {\n",
    "        'method': 'largest_polygon',\n",
    "        'original_records': int(original_records),\n",
    "        'filtered_records': int(filtered_records),\n",
    "        'records_removed': int(original_records - filtered_records),\n",
    "        'multipolygon_topaz_ids': int(multipolygon_count)\n",
    "    }\n",
    "    \n",
    "    return filtered_data, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8ac12",
   "metadata": {},
   "source": [
    "#### Usage Notes\n",
    "\n",
    "The `process_single_watershed` function now includes two optional parameters:\n",
    "\n",
    "1. **`filter_missing` (bool, default=True)**: Automatically filters out subcatchments with any `-9999` values\n",
    "   - Set to `False` to keep all data including subcatchments with missing values\n",
    "   \n",
    "2. **`multipolygon_method` (str, default='weighted')**: Chooses how to handle multipolygons\n",
    "   - `'weighted'`: Area-weighted aggregation (more accurate)\n",
    "   - `'largest'`: Use only the largest polygon (simpler, faster)\n",
    "   \n",
    "Processing statistics for both operations are saved in the metadata JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef5203",
   "metadata": {},
   "source": [
    "**Example usage:**\n",
    "\n",
    "```python\n",
    "# Option 1: Use defaults (filter missing values, weighted aggregation)\n",
    "metadata = await process_single_watershed(\n",
    "    watershed_name='my-watershed',\n",
    "    geojson_url='https://example.com/data.geojson',\n",
    "    dataset='OPENET_CONUS',\n",
    "    variable='et_ensemble_mad',\n",
    "    start_date='2020-01-01',\n",
    "    end_date='2023-12-31'\n",
    ")\n",
    "\n",
    "# Option 2: Keep missing values, use largest polygon method\n",
    "metadata = await process_single_watershed(\n",
    "    watershed_name='my-watershed',\n",
    "    geojson_url='https://example.com/data.geojson',\n",
    "    dataset='OPENET_CONUS',\n",
    "    variable='et_ensemble_mad',\n",
    "    start_date='2020-01-01',\n",
    "    end_date='2023-12-31',\n",
    "    filter_missing=False,\n",
    "    multipolygon_method='largest'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776e27a",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "Process watersheds from the manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bf11c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_single_watershed(\n",
    "    watershed_name: str,\n",
    "    geojson_url: str,\n",
    "    dataset: str = 'OPENET_CONUS',\n",
    "    variable: str = 'et_ensemble_mad',\n",
    "    start_date: str = '2023-01-01',\n",
    "    end_date: str = '2023-12-31',\n",
    "    area_reducer: str = 'mean',\n",
    "    max_concurrent: int = 10,\n",
    "    output_dir: str = 'data',\n",
    "    filter_missing: bool = True,\n",
    "    multipolygon_method: str = 'weighted'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Process all subcatchments for a single watershed and save results.\n",
    "    \n",
    "    Parameters:\n",
    "    - watershed_name: Name of the watershed\n",
    "    - geojson_url: URL to the watershed's GeoJSON file\n",
    "    - dataset: Dataset name (default: 'OPENET_CONUS')\n",
    "    - variable: ET variable name (default: 'et_ensemble_mad')\n",
    "    - start_date: Start date (YYYY-MM-DD)\n",
    "    - end_date: End date (YYYY-MM-DD)\n",
    "    - area_reducer: Spatial aggregation method (default: 'mean')\n",
    "    - max_concurrent: Maximum concurrent requests (default: 10)\n",
    "    - output_dir: Directory to save output files (default: 'data')\n",
    "    - filter_missing: Whether to filter out subcatchments with missing values (default: True)\n",
    "    - multipolygon_method: Method for handling multipolygons - 'weighted' or 'largest' (default: 'weighted')\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing watershed: {watershed_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load subcatchments\n",
    "    logger.info(f\"Loading subcatchments for {watershed_name}...\")\n",
    "    subcatchments_df = await get_subcatchments(geojson_url, verbose=VERBOSE)\n",
    "    \n",
    "    if subcatchments_df is None or len(subcatchments_df) == 0:\n",
    "        print(f\"⚠ Failed to load subcatchments for {watershed_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded {len(subcatchments_df)} subcatchments\")\n",
    "    \n",
    "    # Fetch ET data\n",
    "    et_data_df, fetch_stats = await fetch_all_et_data(\n",
    "        subcatchments_df=subcatchments_df,\n",
    "        dataset=dataset,\n",
    "        variable=variable,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        area_reducer=area_reducer,\n",
    "        max_concurrent=max_concurrent\n",
    "    )\n",
    "    \n",
    "    # Filter out missing values if requested\n",
    "    missing_value_stats = None\n",
    "    if filter_missing and len(et_data_df) > 0:\n",
    "        print(f\"\\nFiltering subcatchments with missing values...\")\n",
    "        et_data_df, subcatchments_df, missing_value_stats = filter_out_missing_values(\n",
    "            et_data_df, subcatchments_df, missing_value=-9999.0\n",
    "        )\n",
    "        print(f\"  Removed {missing_value_stats['subcatchments_with_missing']} subcatchments with missing values\")\n",
    "        print(f\"  Retained {missing_value_stats['subcatchments_retained']} subcatchments ({missing_value_stats['records_retained']} records)\")\n",
    "    \n",
    "    # Handle multipolygons if requested\n",
    "    multipolygon_stats = None\n",
    "    if len(et_data_df) > 0:\n",
    "        # First identify if there are multipolygons\n",
    "        mp_info = identify_multipolygons(subcatchments_df)\n",
    "        if mp_info['multipolygon_topaz_ids'] > 0:\n",
    "            print(f\"\\nAggregating multipolygons using '{multipolygon_method}' method...\")\n",
    "            print(f\"  Found {mp_info['multipolygon_topaz_ids']} multipolygons (out of {mp_info['total_unique_topaz_ids']} total)\")\n",
    "            \n",
    "            if multipolygon_method == 'weighted':\n",
    "                et_data_df, multipolygon_stats = aggregate_multipolygons_weighted(\n",
    "                    et_data_df, subcatchments_df\n",
    "                )\n",
    "            elif multipolygon_method == 'largest':\n",
    "                et_data_df, multipolygon_stats = aggregate_multipolygons_largest(\n",
    "                    et_data_df, subcatchments_df\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  ⚠ Unknown multipolygon_method '{multipolygon_method}', skipping aggregation\")\n",
    "            \n",
    "            if multipolygon_stats:\n",
    "                print(f\"  Aggregated from {multipolygon_stats['original_records']} to {multipolygon_stats.get('aggregated_records', multipolygon_stats.get('filtered_records', 0))} records\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Sanitize watershed name for filename\n",
    "    safe_name = watershed_name.replace(';;', '_').replace('/', '_')\n",
    "    output_file = os.path.join(output_dir, f'et_data_{safe_name}.parquet')\n",
    "    \n",
    "    if len(et_data_df) > 0:\n",
    "        # Add year and month columns for consistency with exploration notebook\n",
    "        et_data_df['year'] = pd.to_datetime(et_data_df['date']).dt.year\n",
    "        et_data_df['month'] = pd.to_datetime(et_data_df['date']).dt.month\n",
    "        et_data_df['model'] = variable\n",
    "        \n",
    "        # Reorder columns: topaz_id, year, month, model, value (rename et_value to value)\n",
    "        et_data_df = et_data_df.rename(columns={'et_value': 'value'})\n",
    "        et_data_df = et_data_df[['topaz_id', 'year', 'month', 'model', 'value']]\n",
    "        \n",
    "        # Save to parquet\n",
    "        et_data_df.to_parquet(output_file, index=False, engine='pyarrow', compression='snappy')\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    else:\n",
    "        file_size_mb = 0\n",
    "        output_file = None\n",
    "        print(f\"⚠ No data collected for {watershed_name}\")\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        'watershed_name': watershed_name,\n",
    "        'date_created': pd.Timestamp.now().isoformat(),\n",
    "        'num_records': int(len(et_data_df)),\n",
    "        'num_subcatchments': int(et_data_df['topaz_id'].nunique()) if len(et_data_df) > 0 else 0,\n",
    "        'models': [variable],\n",
    "        'date_range': {\n",
    "            'start': start_date,\n",
    "            'end': end_date\n",
    "        },\n",
    "        'years': [int(y) for y in sorted(et_data_df['year'].unique())] if len(et_data_df) > 0 else [],\n",
    "        'dataset': dataset,\n",
    "        'variable': variable,\n",
    "        'area_reducer': area_reducer,\n",
    "        'fetch_stats': fetch_stats,\n",
    "        'filter_missing': filter_missing,\n",
    "        'missing_value_stats': missing_value_stats,\n",
    "        'multipolygon_method': multipolygon_method,\n",
    "        'multipolygon_stats': multipolygon_stats\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = os.path.join(output_dir, f'et_data_{safe_name}_metadata.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  ✓ Complete: {len(et_data_df):,} records\")\n",
    "    print(f\"  Unique subcatchments: {metadata['num_subcatchments']}\")\n",
    "    print(f\"  API Success rate: {fetch_stats['success_rate']:.1f}% ({fetch_stats['successful']}/{fetch_stats['total']})\")\n",
    "    if fetch_stats['failed'] > 0:\n",
    "        print(f\"  ⚠ {fetch_stats['failed']} subcatchments failed after retries\")\n",
    "    if missing_value_stats:\n",
    "        print(f\"  Missing value filtering: {missing_value_stats['subcatchments_with_missing']} subcatchments removed\")\n",
    "    if multipolygon_stats:\n",
    "        print(f\"  Multipolygon aggregation: {multipolygon_stats['method']}\")\n",
    "    if output_file:\n",
    "        print(f\"  Saved: {output_file} ({file_size_mb:.2f} MB)\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d633df40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 395 watersheds from manifest\n",
      "\n",
      "First 5 watersheds:\n",
      "  - batch;;nasa-roses-2025;;wa-0\n",
      "  - batch;;nasa-roses-2025;;wa-1\n",
      "  - batch;;nasa-roses-2025;;wa-2\n",
      "  - batch;;nasa-roses-2025;;wa-3\n",
      "  - batch;;nasa-roses-2025;;wa-4\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the watershed manifest\n",
    "with open(SUBCATCHMENTS_MANIFEST, 'r') as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "watersheds = manifest['Subcatchments']\n",
    "print(f\"Loaded {len(watersheds)} watersheds from manifest\")\n",
    "print(f\"\\nFirst 5 watersheds:\")\n",
    "for w in watersheds[:5]:\n",
    "    print(f\"  - {w['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading subcatchments for batch;;nasa-roses-2025;;wa-1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Date range: 2020-01-01 to 2023-12-31\n",
      "  Dataset: OPENET_CONUS\n",
      "  Variable: et_ensemble_mad\n",
      "  Max concurrent: 100\n",
      "============================================================\n",
      "\n",
      "Processing 3 selected watershed(s):\n",
      "  - batch;;nasa-roses-2025;;wa-1\n",
      "  - batch;;nasa-roses-2025;;wa-2\n",
      "  - batch;;nasa-roses-2025;;wa-3\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing watershed: batch;;nasa-roses-2025;;wa-1\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 12 subcatchment features (geometries)\n",
      "INFO:root:Found 9 unique TopazIDs\n",
      "INFO:root:Sample TopazIDs: [32, 33, 41, 42, 51]\n",
      "INFO:root:Found 9 unique TopazIDs\n",
      "INFO:root:Sample TopazIDs: [32, 33, 41, 42, 51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 subcatchments\n",
      "Starting concurrent fetch for 12 subcatchments...\n",
      "Max concurrent requests: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ET data: 100%|██████████| 12/12 [00:33<00:00,  2.83s/it]\n",
      "INFO:root:Loading subcatchments for batch;;nasa-roses-2025;;wa-2...\n",
      "\n",
      "INFO:root:Loading subcatchments for batch;;nasa-roses-2025;;wa-2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed in 33.97 seconds (2.83s per subcatchment)\n",
      "Success: 12/12 (100.0%)\n",
      "\n",
      "Filtering subcatchments with missing values...\n",
      "  Removed 3 subcatchments with missing values\n",
      "  Retained 6 subcatchments (336 records)\n",
      "\n",
      "Aggregating multipolygons using 'weighted' method...\n",
      "  Found 1 multipolygons (out of 6 total)\n",
      "Calculating polygon areas...\n",
      "  Aggregated from 336 to 288 records\n",
      "\n",
      "  ✓ Complete: 288 records\n",
      "  Unique subcatchments: 6\n",
      "  API Success rate: 100.0% (12/12)\n",
      "  Missing value filtering: 3 subcatchments removed\n",
      "  Multipolygon aggregation: area_weighted\n",
      "  Saved: data/et_data_batch_nasa-roses-2025_wa-1.parquet (0.01 MB)\n",
      "\n",
      "============================================================\n",
      "Processing watershed: batch;;nasa-roses-2025;;wa-2\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 39 subcatchment features (geometries)\n",
      "INFO:root:Found 29 unique TopazIDs\n",
      "INFO:root:Sample TopazIDs: [22, 23, 31, 32, 33]\n",
      "INFO:root:Found 29 unique TopazIDs\n",
      "INFO:root:Sample TopazIDs: [22, 23, 31, 32, 33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 39 subcatchments\n",
      "Starting concurrent fetch for 39 subcatchments...\n",
      "Max concurrent requests: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ET data: 100%|██████████| 39/39 [01:08<00:00,  1.76s/it]\n",
      "INFO:root:Loading subcatchments for batch;;nasa-roses-2025;;wa-3...\n",
      "\n",
      "INFO:root:Loading subcatchments for batch;;nasa-roses-2025;;wa-3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed in 68.76 seconds (1.76s per subcatchment)\n",
      "Success: 39/39 (100.0%)\n",
      "\n",
      "Filtering subcatchments with missing values...\n",
      "  Removed 27 subcatchments with missing values\n",
      "  Retained 2 subcatchments (96 records)\n",
      "\n",
      "  ✓ Complete: 96 records\n",
      "  Unique subcatchments: 2\n",
      "  API Success rate: 100.0% (39/39)\n",
      "  Missing value filtering: 27 subcatchments removed\n",
      "  Saved: data/et_data_batch_nasa-roses-2025_wa-2.parquet (0.00 MB)\n",
      "\n",
      "============================================================\n",
      "Processing watershed: batch;;nasa-roses-2025;;wa-3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 21 subcatchment features (geometries)\n",
      "INFO:root:Found 13 unique TopazIDs\n",
      "INFO:root:Sample TopazIDs: [22, 23, 31, 32, 33]\n",
      "INFO:root:Found 13 unique TopazIDs\n",
      "INFO:root:Sample TopazIDs: [22, 23, 31, 32, 33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21 subcatchments\n",
      "Starting concurrent fetch for 21 subcatchments...\n",
      "Max concurrent requests: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ET data:  48%|████▊     | 10/21 [00:28<00:31,  2.88s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[163]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m all_metadata = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m watershed \u001b[38;5;129;01min\u001b[39;00m watersheds_to_process:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     metadata = \u001b[38;5;28;01mawait\u001b[39;00m process_single_watershed(\n\u001b[32m     55\u001b[39m         watershed_name=watershed[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     56\u001b[39m         geojson_url=watershed[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     57\u001b[39m         dataset=DATASET,\n\u001b[32m     58\u001b[39m         variable=VARIABLE,\n\u001b[32m     59\u001b[39m         start_date=START_DATE,\n\u001b[32m     60\u001b[39m         end_date=END_DATE,\n\u001b[32m     61\u001b[39m         max_concurrent=MAX_CONCURRENT\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[32m     64\u001b[39m         all_metadata.append(metadata)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[161]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mprocess_single_watershed\u001b[39m\u001b[34m(watershed_name, geojson_url, dataset, variable, start_date, end_date, area_reducer, max_concurrent, output_dir, filter_missing, multipolygon_method)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(subcatchments_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m subcatchments\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Fetch ET data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m et_data_df, fetch_stats = \u001b[38;5;28;01mawait\u001b[39;00m fetch_all_et_data(\n\u001b[32m     49\u001b[39m     subcatchments_df=subcatchments_df,\n\u001b[32m     50\u001b[39m     dataset=dataset,\n\u001b[32m     51\u001b[39m     variable=variable,\n\u001b[32m     52\u001b[39m     start_date=start_date,\n\u001b[32m     53\u001b[39m     end_date=end_date,\n\u001b[32m     54\u001b[39m     area_reducer=area_reducer,\n\u001b[32m     55\u001b[39m     max_concurrent=max_concurrent\n\u001b[32m     56\u001b[39m )\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Filter out missing values if requested\u001b[39;00m\n\u001b[32m     59\u001b[39m missing_value_stats = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mfetch_all_et_data\u001b[39m\u001b[34m(subcatchments_df, dataset, variable, start_date, end_date, area_reducer, max_concurrent)\u001b[39m\n\u001b[32m     64\u001b[39m responses = []\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m coro \u001b[38;5;129;01min\u001b[39;00m tqdm.as_completed(tasks, total=\u001b[38;5;28mlen\u001b[39m(tasks), desc=\u001b[33m\"\u001b[39m\u001b[33mFetching ET data\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     67\u001b[39m     responses.append(result)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Process results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/asyncio/tasks.py:630\u001b[39m, in \u001b[36m_AsCompletedIterator._wait_for_one\u001b[39m\u001b[34m(self, resolve)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wait_for_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, resolve=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    627\u001b[39m     \u001b[38;5;66;03m# Wait for the next future to be done and return it unless resolve is\u001b[39;00m\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# set, in which case return either the result of the future or raise\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# an exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     f = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._done.get()\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    632\u001b[39m         \u001b[38;5;66;03m# Dummy value from _handle_timeout().\u001b[39;00m\n\u001b[32m    633\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/asyncio/queues.py:186\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28mself\u001b[39m._getters.append(getter)\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m getter\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    188\u001b[39m     getter.cancel()  \u001b[38;5;66;03m# Just in case getter is not done yet.\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n",
      "ERROR:root:Exception during ET API request (after retries): RuntimeError: Session is closed\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2023-12-31'\n",
    "DATASET = 'OPENET_CONUS'\n",
    "VARIABLE = 'et_ensemble_mad'\n",
    "MAX_CONCURRENT = 100  # Adjust based on API rate limits\n",
    "\n",
    "# Watershed selection\n",
    "# Option 1: Process all watersheds\n",
    "PROCESS_ALL_WATERSHEDS = False\n",
    "\n",
    "# Option 2: Process specific watersheds by name or index\n",
    "# Examples:\n",
    "#   SELECTED_WATERSHEDS = ['batch;;nasa-roses-2025;;wa-0', 'batch;;nasa-roses-2025;;wa-1']\n",
    "#   SELECTED_WATERSHEDS = [0, 1, 2]  # Process first 3 watersheds\n",
    "SELECTED_WATERSHEDS = [1, 2, 3]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Dataset: {DATASET}\")\n",
    "print(f\"  Variable: {VARIABLE}\")\n",
    "print(f\"  Max concurrent: {MAX_CONCURRENT}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Determine which watersheds to process\n",
    "if PROCESS_ALL_WATERSHEDS:\n",
    "    watersheds_to_process = watersheds\n",
    "    print(f\"Processing ALL {len(watersheds)} watersheds\")\n",
    "else:\n",
    "    # Handle both name-based and index-based selection\n",
    "    watersheds_to_process = []\n",
    "    for selection in SELECTED_WATERSHEDS:\n",
    "        if isinstance(selection, int):\n",
    "            if 0 <= selection < len(watersheds):\n",
    "                watersheds_to_process.append(watersheds[selection])\n",
    "            else:\n",
    "                print(f\"⚠ Warning: Index {selection} out of range (0-{len(watersheds)-1})\")\n",
    "        elif isinstance(selection, str):\n",
    "            matching = [w for w in watersheds if w['name'] == selection]\n",
    "            if matching:\n",
    "                watersheds_to_process.append(matching[0])\n",
    "            else:\n",
    "                print(f\"⚠ Warning: Watershed '{selection}' not found in manifest\")\n",
    "    \n",
    "    print(f\"Processing {len(watersheds_to_process)} selected watershed(s):\")\n",
    "    for w in watersheds_to_process:\n",
    "        print(f\"  - {w['name']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Process each watershed\n",
    "all_metadata = []\n",
    "for watershed in watersheds_to_process:\n",
    "    metadata = await process_single_watershed(\n",
    "        watershed_name=watershed['name'],\n",
    "        geojson_url=watershed['url'],\n",
    "        dataset=DATASET,\n",
    "        variable=VARIABLE,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        max_concurrent=MAX_CONCURRENT\n",
    "    )\n",
    "    if metadata:\n",
    "        all_metadata.append(metadata)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL WATERSHEDS COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Processed: {len(all_metadata)}/{len(watersheds_to_process)} watersheds\")\n",
    "\n",
    "if all_metadata:\n",
    "    total_records = sum(m['num_records'] for m in all_metadata)\n",
    "    total_subcatchments = sum(m['num_subcatchments'] for m in all_metadata)\n",
    "    \n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(f\"Total subcatchments: {total_subcatchments:,}\")\n",
    "    \n",
    "    # Save summary metadata\n",
    "    summary_file = 'data/processing_summary.json'\n",
    "    summary = {\n",
    "        'date_created': pd.Timestamp.now().isoformat(),\n",
    "        'total_watersheds_processed': len(all_metadata),\n",
    "        'total_records': int(total_records),\n",
    "        'total_subcatchments': int(total_subcatchments),\n",
    "        'date_range': {'start': START_DATE, 'end': END_DATE},\n",
    "        'dataset': DATASET,\n",
    "        'variable': VARIABLE,\n",
    "        'watersheds': all_metadata\n",
    "    }\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"\\nSummary saved: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9820f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watershed: batch;;nasa-roses-2025;;wa-0\n",
      "============================================================\n",
      "\n",
      "Dataset Summary:\n",
      "  Total records: 10,224\n",
      "  Unique subcatchments: 213\n",
      "  Date range: 2020-2023\n",
      "\n",
      "Sample data:\n",
      "   topaz_id  year  month            model     value\n",
      "0        61  2020      1  et_ensemble_mad    3.9896\n",
      "1        61  2020      2  et_ensemble_mad   15.7719\n",
      "2        61  2020      3  et_ensemble_mad   34.0795\n",
      "3        61  2020      4  et_ensemble_mad   66.8073\n",
      "4        61  2020      5  et_ensemble_mad   94.5728\n",
      "5        61  2020      6  et_ensemble_mad   95.0670\n",
      "6        61  2020      7  et_ensemble_mad  116.2993\n",
      "7        61  2020      8  et_ensemble_mad  106.3052\n",
      "8        61  2020      9  et_ensemble_mad   69.5023\n",
      "9        61  2020     10  et_ensemble_mad   27.6604\n",
      "\n",
      "Summary Statistics:\n",
      "count    10224.000000\n",
      "mean        59.011291\n",
      "std         46.984771\n",
      "min          0.000000\n",
      "25%         13.610937\n",
      "50%         44.871000\n",
      "75%        104.521325\n",
      "max        167.000000\n",
      "Name: value, dtype: float64\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 45 column 31 (char 1066)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(metadata_file):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(metadata_file, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         metadata = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMetadata:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[33m'\u001b[39m\u001b[33mdate_created\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/json/__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/json/decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/json/decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 45 column 31 (char 1066)"
     ]
    }
   ],
   "source": [
    "# Load data for a specific watershed\n",
    "watershed_name = 'batch;;nasa-roses-2025;;wa-0'  # Change to your watershed name\n",
    "safe_name = watershed_name.replace(';;', '_').replace('/', '_')\n",
    "data_file = f'data/et_data_{safe_name}.parquet'\n",
    "metadata_file = f'data/et_data_{safe_name}_metadata.json'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(data_file):\n",
    "    # Load the data\n",
    "    df = pd.read_parquet(data_file)\n",
    "    \n",
    "    print(f\"Watershed: {watershed_name}\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Total records: {len(df):,}\")\n",
    "    print(f\"  Unique subcatchments: {df['topaz_id'].nunique()}\")\n",
    "    print(f\"  Date range: {df['year'].min()}-{df['year'].max()}\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(df[['topaz_id', 'year', 'month', 'model', 'value']].head(10))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(df['value'].describe())\n",
    "    \n",
    "    # Load and display metadata\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Created: {metadata['date_created']}\")\n",
    "        print(f\"  Dataset: {metadata['dataset']}\")\n",
    "        print(f\"  Variable: {metadata['variable']}\")\n",
    "else:\n",
    "    print(f\"No data found for watershed: {watershed_name}\")\n",
    "    print(f\"Looking for: {data_file}\")\n",
    "    print(f\"\\nAvailable watershed files:\")\n",
    "    if os.path.exists('data'):\n",
    "        parquet_files = [f for f in os.listdir('data') if f.endswith('.parquet') and f.startswith('et_data_')]\n",
    "        for f in parquet_files:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(\"  (data directory does not exist yet)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"To load data for a different watershed:\")\n",
    "print(f\"  1. Change the watershed_name variable above\")\n",
    "print(f\"  2. Re-run this cell\")\n",
    "print(f\"\\nTo load the processing summary:\")\n",
    "print(f\"  with open('data/processing_summary.json', 'r') as f:\")\n",
    "print(f\"      summary = json.load(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece1796",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Load and analyze the processed watershed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbcf96",
   "metadata": {},
   "source": [
    "## Random Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c98bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'et_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Diagnostic: Check the et_data DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43met_data\u001b[49m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00met_data.columns.tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame empty: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00met_data.empty\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'et_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check the et_data DataFrame\n",
    "print(f\"DataFrame shape: {et_data.shape}\")\n",
    "print(f\"DataFrame columns: {et_data.columns.tolist()}\")\n",
    "print(f\"DataFrame empty: {et_data.empty}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(et_data.head())\n",
    "\n",
    "if et_data.empty:\n",
    "    print(\"\\n⚠️  WARNING: No data was collected!\")\n",
    "    print(\"Possible reasons:\")\n",
    "    print(\"1. All API responses contained only NoData (-9999.0) values\")\n",
    "    print(\"2. API responses didn't contain 'data' key\")\n",
    "    print(\"3. Date range has no available data\")\n",
    "    print(\"\\nTry running a single request manually to check the API response format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential vs Concurrent Performance Comparison\n",
    "async def compare_sequential_vs_concurrent(num_tests=10):\n",
    "    \"\"\"\n",
    "    Compare sequential vs concurrent execution to measure real performance gains.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from tqdm.asyncio import tqdm\n",
    "    \n",
    "    sample_df = subcatchment_data.head(num_tests)\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"PERFORMANCE COMPARISON: Sequential vs Concurrent\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Testing with {num_tests} subcatchments...\\n\")\n",
    "    \n",
    "    # ========== SEQUENTIAL TEST ==========\n",
    "    print(\"🐌 Running SEQUENTIAL test...\")\n",
    "    sequential_start = time.time()\n",
    "    sequential_results = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            req_start = time.time()\n",
    "            response = await fetch_et_timeseries(\n",
    "                session=session,\n",
    "                geometry=row['geometry'],\n",
    "                dataset='OPENET_CONUS',\n",
    "                variable='et_ensemble_mad',\n",
    "                start_date='2023-01-01',\n",
    "                end_date='2023-01-31',\n",
    "                area_reducer='mean'\n",
    "            )\n",
    "            req_time = time.time() - req_start\n",
    "            sequential_results.append((idx, req_time, response is not None))\n",
    "            print(f\"  Request {idx + 1}/{num_tests} completed in {req_time:.2f}s\")\n",
    "    \n",
    "    sequential_time = time.time() - sequential_start\n",
    "    sequential_request_times = [r[1] for r in sequential_results]\n",
    "    sequential_success = sum(1 for r in sequential_results if r[2])\n",
    "    \n",
    "    print(f\"\\n✓ Sequential completed in {sequential_time:.2f}s\")\n",
    "    print(f\"  Success rate: {sequential_success}/{num_tests}\")\n",
    "    print(f\"  Avg per request: {sum(sequential_request_times)/len(sequential_request_times):.2f}s\\n\")\n",
    "    \n",
    "    # ========== CONCURRENT TEST ==========\n",
    "    print(\"🚀 Running CONCURRENT test...\")\n",
    "    concurrent_start = time.time()\n",
    "    concurrent_results = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        semaphore = asyncio.Semaphore(50)\n",
    "        \n",
    "        async def timed_fetch(idx, row):\n",
    "            req_start = time.time()\n",
    "            async with semaphore:\n",
    "                response = await fetch_et_timeseries(\n",
    "                    session=session,\n",
    "                    geometry=row['geometry'],\n",
    "                    dataset='OPENET_CONUS',\n",
    "                    variable='et_ensemble_mad',\n",
    "                    start_date='2023-01-01',\n",
    "                    end_date='2023-01-31',\n",
    "                    area_reducer='mean'\n",
    "                )\n",
    "            req_time = time.time() - req_start\n",
    "            return idx, req_time, response is not None\n",
    "        \n",
    "        tasks = [\n",
    "            asyncio.create_task(timed_fetch(idx, row))\n",
    "            for idx, row in sample_df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        for coro in tqdm.as_completed(tasks, total=len(tasks), desc=\"  Processing\"):\n",
    "            result = await coro\n",
    "            concurrent_results.append(result)\n",
    "    \n",
    "    concurrent_time = time.time() - concurrent_start\n",
    "    concurrent_request_times = [r[1] for r in concurrent_results]\n",
    "    concurrent_success = sum(1 for r in concurrent_results if r[2])\n",
    "    \n",
    "    print(f\"\\n✓ Concurrent completed in {concurrent_time:.2f}s\")\n",
    "    print(f\"  Success rate: {concurrent_success}/{num_tests}\")\n",
    "    print(f\"  Avg per request: {sum(concurrent_request_times)/len(concurrent_request_times):.2f}s\\n\")\n",
    "    \n",
    "    # ========== RESULTS SUMMARY ==========\n",
    "    speedup = sequential_time / concurrent_time\n",
    "    time_saved = sequential_time - concurrent_time\n",
    "    time_saved_pct = (time_saved / sequential_time) * 100\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"RESULTS SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sequential time:  {sequential_time:.2f}s\")\n",
    "    print(f\"Concurrent time:  {concurrent_time:.2f}s\")\n",
    "    print(f\"Time saved:       {time_saved:.2f}s ({time_saved_pct:.1f}%)\")\n",
    "    print(f\"Speedup factor:   {speedup:.2f}x\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    \n",
    "    # Extrapolate to full dataset\n",
    "    full_dataset_size = len(subcatchment_data)\n",
    "    estimated_sequential = (sequential_time / num_tests) * full_dataset_size\n",
    "    estimated_concurrent = (concurrent_time / num_tests) * full_dataset_size\n",
    "    \n",
    "    print(f\"EXTRAPOLATION FOR {full_dataset_size} SUBCATCHMENTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Estimated sequential time:  {estimated_sequential/60:.1f} minutes ({estimated_sequential/3600:.2f} hours)\")\n",
    "    print(f\"Estimated concurrent time:  {estimated_concurrent/60:.1f} minutes ({estimated_concurrent/3600:.2f} hours)\")\n",
    "    print(f\"Estimated time saved:       {(estimated_sequential - estimated_concurrent)/60:.1f} minutes\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if speedup > 5:\n",
    "        print(\"🎉 Excellent! Concurrency provides significant performance gains!\")\n",
    "    elif speedup > 2:\n",
    "        print(\"👍 Good! Concurrency is working well.\")\n",
    "    else:\n",
    "        print(\"⚠️  Limited concurrency gains. API may be rate-limiting.\")\n",
    "\n",
    "# Run comparison test\n",
    "await compare_sequential_vs_concurrent(num_tests=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff9058",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "The comparison test shows **limited concurrency gains** (1.28x speedup). This indicates the ClimateEngine API is **rate-limiting concurrent requests**:\n",
    "\n",
    "- **Sequential**: Each request takes ~2s (normal API response time)\n",
    "- **Concurrent**: Each request takes ~8s (4x slower due to throttling)\n",
    "\n",
    "**Why this happens:**\n",
    "- The API detects multiple concurrent requests from the same source\n",
    "- It throttles responses to prevent overwhelming the server\n",
    "- This is common for public APIs\n",
    "\n",
    "**Options to improve performance:**\n",
    "1. **Reduce concurrency** - Try `max_concurrent=5` or `10` instead of `50`\n",
    "2. **Add delays** - Space out requests with small delays\n",
    "3. **Accept the limitation** - 1.28x is still faster than sequential\n",
    "4. **Contact API provider** - Ask about rate limits and best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413beeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different concurrency levels to find optimal setting\n",
    "async def test_optimal_concurrency(num_tests=10):\n",
    "    \"\"\"\n",
    "    Test different concurrency levels to find the sweet spot.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from tqdm.asyncio import tqdm\n",
    "    \n",
    "    sample_df = subcatchment_data.head(num_tests)\n",
    "    concurrency_levels = [1, 3, 5, 10, 20, 50]\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TESTING OPTIMAL CONCURRENCY LEVEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Testing {num_tests} requests at different concurrency levels...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for max_concurrent in concurrency_levels:\n",
    "        print(f\"Testing max_concurrent={max_concurrent}...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            semaphore = asyncio.Semaphore(max_concurrent)\n",
    "            \n",
    "            async def fetch_one(idx, row):\n",
    "                async with semaphore:\n",
    "                    return await fetch_et_timeseries(\n",
    "                        session=session,\n",
    "                        geometry=row['geometry'],\n",
    "                        dataset='OPENET_CONUS',\n",
    "                        variable='et_ensemble_mad',\n",
    "                        start_date='2023-01-01',\n",
    "                        end_date='2023-01-31',\n",
    "                        area_reducer='mean'\n",
    "                    )\n",
    "            \n",
    "            tasks = [\n",
    "                asyncio.create_task(fetch_one(idx, row))\n",
    "                for idx, row in sample_df.iterrows()\n",
    "            ]\n",
    "            \n",
    "            responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        success_count = sum(1 for r in responses if r is not None and not isinstance(r, Exception))\n",
    "        avg_per_request = elapsed / num_tests\n",
    "        \n",
    "        results.append({\n",
    "            'max_concurrent': max_concurrent,\n",
    "            'total_time': elapsed,\n",
    "            'avg_per_request': avg_per_request,\n",
    "            'success_rate': success_count / num_tests\n",
    "        })\n",
    "        \n",
    "        print(f\"{elapsed:.2f}s (avg {avg_per_request:.2f}s/req, {success_count}/{num_tests} success)\")\n",
    "    \n",
    "    # Find optimal\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Level':<10} {'Total Time':<15} {'Avg/Request':<15} {'Success Rate'}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    best_result = min(results, key=lambda x: x['total_time'])\n",
    "    \n",
    "    for r in results:\n",
    "        marker = \"  ⭐ BEST\" if r == best_result else \"\"\n",
    "        print(f\"{r['max_concurrent']:<10} {r['total_time']:>6.2f}s{' '*7} \"\n",
    "              f\"{r['avg_per_request']:>6.2f}s{' '*7} \"\n",
    "              f\"{r['success_rate']*100:>5.1f}%{marker}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RECOMMENDATION: Use max_concurrent={best_result['max_concurrent']}\")\n",
    "    print(f\"Expected time for 555 subcatchments: {(best_result['avg_per_request'] * 555)/60:.1f} minutes\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Run optimization test\n",
    "await test_optimal_concurrency(num_tests=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitive concurrency test with timing overlap analysis\n",
    "async def prove_concurrency_works(num_tests=20):\n",
    "    \"\"\"\n",
    "    Definitive test that records exact start/end times to prove requests overlap.\n",
    "    If requests truly run concurrently, many should be IN PROGRESS at the same time.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    sample_df = subcatchment_data.head(num_tests)\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"DEFINITIVE CONCURRENCY TEST\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Test with high concurrency\n",
    "    max_concurrent = 20\n",
    "    print(f\"Running {num_tests} requests with max_concurrent={max_concurrent}\\n\")\n",
    "    \n",
    "    request_events = []  # Track when each request starts and ends\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        async def fetch_with_timing(idx, row):\n",
    "            start_time = time.time()\n",
    "            async with semaphore:\n",
    "                print(f\"[{time.time():.2f}] Request {idx+1} STARTED\")\n",
    "                response = await fetch_et_timeseries(\n",
    "                    session=session,\n",
    "                    geometry=row['geometry'],\n",
    "                    dataset='OPENET_CONUS',\n",
    "                    variable='et_ensemble_mad',\n",
    "                    start_date='2023-01-01',\n",
    "                    end_date='2023-01-31',\n",
    "                    area_reducer='mean'\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                print(f\"[{end_time:.2f}] Request {idx+1} FINISHED (took {duration:.2f}s)\")\n",
    "                return {\n",
    "                    'id': idx + 1,\n",
    "                    'start': start_time,\n",
    "                    'end': end_time,\n",
    "                    'duration': duration,\n",
    "                    'success': response is not None\n",
    "                }\n",
    "        \n",
    "        test_start = time.time()\n",
    "        tasks = [\n",
    "            asyncio.create_task(fetch_with_timing(idx, row))\n",
    "            for idx, row in sample_df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        test_end = time.time()\n",
    "    \n",
    "    total_time = test_end - test_start\n",
    "    \n",
    "    # Analyze overlap\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"OVERLAP ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # For each point in time, count how many requests were active\n",
    "    time_points = []\n",
    "    for r in results:\n",
    "        time_points.append((r['start'], 1))   # +1 active request\n",
    "        time_points.append((r['end'], -1))     # -1 active request\n",
    "    \n",
    "    time_points.sort()\n",
    "    \n",
    "    max_concurrent_actual = 0\n",
    "    current_concurrent = 0\n",
    "    \n",
    "    for t, delta in time_points:\n",
    "        current_concurrent += delta\n",
    "        max_concurrent_actual = max(max_concurrent_actual, current_concurrent)\n",
    "    \n",
    "    avg_duration = sum(r['duration'] for r in results) / len(results)\n",
    "    success_count = sum(1 for r in results if r['success'])\n",
    "    \n",
    "    print(f\"Total time:                {total_time:.2f}s\")\n",
    "    print(f\"Success rate:              {success_count}/{num_tests}\")\n",
    "    print(f\"Avg request duration:      {avg_duration:.2f}s\")\n",
    "    print(f\"Max concurrent (observed): {max_concurrent_actual}\")\n",
    "    print(f\"Max concurrent (setting):  {max_concurrent}\")\n",
    "    \n",
    "    # Calculate theoretical times\n",
    "    theoretical_sequential = avg_duration * num_tests\n",
    "    theoretical_concurrent = avg_duration  # If perfect concurrency\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CONCURRENCY PROOF\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"If SEQUENTIAL (1 at a time): Would take {theoretical_sequential:.2f}s\")\n",
    "    print(f\"If CONCURRENT ({max_concurrent} at once): Should take ~{avg_duration:.2f}s\")\n",
    "    print(f\"Actual time:                  {total_time:.2f}s\")\n",
    "    \n",
    "    efficiency = (theoretical_sequential / total_time) / max_concurrent_actual\n",
    "    \n",
    "    print(f\"\\nActual max concurrent:        {max_concurrent_actual} requests\")\n",
    "    print(f\"Concurrency efficiency:       {efficiency*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    if max_concurrent_actual >= 10:\n",
    "        print(f\"✅ CONCURRENCY IS WORKING! {max_concurrent_actual} requests ran simultaneously.\")\n",
    "    elif max_concurrent_actual >= 3:\n",
    "        print(f\"⚠️  LIMITED CONCURRENCY. Only {max_concurrent_actual} requests ran simultaneously.\")\n",
    "        print(f\"   API is likely limiting you to {max_concurrent_actual} concurrent connections.\")\n",
    "    else:\n",
    "        print(f\"❌ CONCURRENCY NOT WORKING. Only {max_concurrent_actual} requests ran simultaneously.\")\n",
    "        print(f\"   This is essentially sequential execution.\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "await prove_concurrency_works(num_tests=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02513c3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
