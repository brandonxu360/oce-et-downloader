{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0b57e0",
   "metadata": {},
   "source": [
    "# Agricultural Fields Data - Timeseries Endpoint\n",
    "\n",
    "#### Data was requested with the following specifications:\n",
    "\n",
    "* Years: 2008-2024\n",
    "* Months: 4, 5, 6, 7, 8, 9\n",
    "* Type: Remote Sensing:\n",
    "    * **Dataset**: Landsat 5/7/8/9 SR – 30 **Variable**: NDVI, MSAVI, NDWI \n",
    "    * **Dataset**: Open ET – 30m – Monthly **Variable**: ETa: eeMETRIC, geeSEBAL, DISALEXI\n",
    "    * **Dataset**: Sentinel 2 SR – 10m – 5day **Variable**: NDVI, MSAVI, NDWI, NDRE, BSI\n",
    " \n",
    "Variable names:\n",
    "L-NDVI, L-MSAVI, L-NDWI,\n",
    "eeMETRIC, geeSEBAL, DISALEXI\n",
    "S-NDVI, S-MSAVI, S-NDWI, S-NDRE, S-BSI\n",
    "\n",
    "*L is appended in front of the variables generated with Landsat and an S in front of those generated with Sentinel. Sentinel will not have data prior to 2017 or so.*\n",
    "\n",
    "The area reducer of median and temporal reducers (by month) of mean and max are requested.\n",
    "\n",
    "#### Notes\n",
    "The [timeseries endpoint](https://docs.climateengine.org/docs/build/html/timeseries.html#rst-timeseries-native-coordinates) is used in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14655860",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AUTHENTICATION\n",
    "load_dotenv()\n",
    "CLIMATE_ENGINE_API_KEY = os.environ.get('CLIMATE_ENGINE_API_KEY')\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json',\n",
    "    'Authorization': CLIMATE_ENGINE_API_KEY\n",
    "}\n",
    "\n",
    "# LOGGING\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, # INFO for useful info, DEBUG for uglier, verbose info\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"climateengine.scraper.timeseries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927bd92",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "### Datasets\n",
    "Available datasets from ClimateEngine API: https://docs.climateengine.org/docs/build/html/datasets.html\n",
    "\n",
    "API Parameters:\n",
    "* Landsat 5/7/8/9 SR - 30m: LANDSAT_SR\n",
    "* OpenET - 30m - Monthly: OPENET_CONUS\n",
    "* Sentinel 2 SR - 10m - 5day: SENTINEL2_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2974937",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['LANDSAT_SR', 'OPENET_CONUS', 'SENTINEL2_SR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220cd77",
   "metadata": {},
   "source": [
    "### Variables\n",
    "The API allows us to see the available variables for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_variables_metadata_dataset_variables_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a20e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_utils import synchronous_fetch_with_retry\n",
    "\n",
    "# All requested variables for the dataset of corresponding index\n",
    "# Most important NDWI variable is NDWI_NIR_SWIR_GAO - vegatation moisture \n",
    "VARIABLES = [ \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao'],\n",
    "    ['et_eemetric', 'et_geesebal', 'et_disalexi'], \n",
    "    ['NDVI', 'MSAVI', 'NDWI_NIR_SWIR_Gao', 'NDRE', 'BSI']]\n",
    "\n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_variables?dataset={dataset}', headers=HEADERS)\n",
    "\n",
    "    api_variables = set(res.get('Data').get('variables'))\n",
    "    missing = set(VARIABLES[i]).difference(api_variables)\n",
    "\n",
    "    if missing:\n",
    "        logger.info(f'{dataset}: ✗ API missing requested variables {missing}')\n",
    "    else:\n",
    "        logger.info(f'{dataset}: ✓ all variables available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9f32",
   "metadata": {},
   "source": [
    "### Dates\n",
    "The API allows us to see the minimum and maximum dates for a given dataset: https://api.climateengine.org/docs#/metadata/metadata_dataset_dates_metadata_dataset_dates_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requested year range for the dataset of corresponding index indicated by a start and end inclusive\n",
    "YEARS = [\n",
    "    [2008, 2024],\n",
    "    [2008, 2024],\n",
    "    [2017, 2024]]\n",
    "\n",
    "# Explicit enumeration of desired months\n",
    "MONTHS = [\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9],\n",
    "    [4, 5, 6, 7, 8, 9]]\n",
    "    \n",
    "for i, dataset in enumerate(DATASETS):\n",
    "    res = synchronous_fetch_with_retry(f'https://api.climateengine.org/metadata/dataset_dates?dataset={dataset}', headers=HEADERS)\n",
    "    \n",
    "    data = res.get('Data')\n",
    "    date_min = int(data['min'][:4])  # Extract year from date string\n",
    "    date_max = int(data['max'][:4])\n",
    "    req_min, req_max = YEARS[i]\n",
    "    available = '✓' if req_min >= date_min and req_max <= date_max else '✗'\n",
    "    logger.info(f'{dataset}: {available} (available: {date_min}-{date_max}, requested: {req_min}-{req_max})') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b7c36",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1125355",
   "metadata": {},
   "source": [
    "### Prepare Agricultural Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AG_FIELDS_URL = 'https://wc.bearhive.duckdns.org/weppcloud/runs/copacetic-note/ag-fields/browse/ag_fields/CSB_2008_2024_Hangman_with_Crop_and_Performance.geojson?raw=true'\n",
    "\n",
    "fields_data = synchronous_fetch_with_retry(AG_FIELDS_URL)\n",
    "\n",
    "# Extract field data\n",
    "fields = []\n",
    "for feature in fields_data['features']:\n",
    "    properties = feature['properties']\n",
    "    field_id = properties.get('field_ID')\n",
    "    geometry = feature['geometry']\n",
    "    \n",
    "    fields.append({\n",
    "        'field_id': field_id,\n",
    "        'geometry': geometry,\n",
    "    })\n",
    "\n",
    "fields_df = pd.DataFrame(fields)\n",
    "logger.info(fields_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb538df",
   "metadata": {},
   "source": [
    "### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b574ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import calendar\n",
    "from tqdm.asyncio import tqdm\n",
    "from scrape_utils import asynchronous_fetch_with_retry\n",
    "\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "async def fetch_data(dataset: str, dataset_index: int, session: aiohttp.ClientSession):\n",
    "    tasks = [\n",
    "        asynchronous_fetch_with_retry(\n",
    "            session=session,\n",
    "            url='https://api.climateengine.org/timeseries/native/coordinates',\n",
    "            semaphore=semaphore,\n",
    "            headers=HEADERS,\n",
    "            params={\n",
    "                'dataset': dataset,\n",
    "                'variable': ','.join(VARIABLES[dataset_index]),\n",
    "                'start_date': f'{YEARS[dataset_index][0]}-{MONTHS[dataset_index][0]:02d}-01',\n",
    "                'end_date': f'{YEARS[dataset_index][1]}-{MONTHS[dataset_index][-1]:02d}-{calendar.monthrange(YEARS[dataset_index][1], MONTHS[dataset_index][-1])[1]}',\n",
    "                'area_reducer': 'median',\n",
    "                'coordinates': json.dumps(row['geometry']['coordinates'])\n",
    "            })\n",
    "        for _, row in fields_df.head(3).iterrows()\n",
    "    ]\n",
    "\n",
    "    return await tqdm.gather(*tasks)\n",
    "\n",
    "def convert_results_to_pandas(results) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    # Order of asyncio results is preserved which allows us to determine field ID\n",
    "    # https://docs.python.org/3/library/asyncio-task.html#running-tasks-concurrently\n",
    "    for i, result in enumerate(results):\n",
    "        field_id = fields_df.iloc[i]['field_id']\n",
    "        \n",
    "        # Each result['Data'] is a list with one item containing the timeseries\n",
    "        if 'Data' in result and len(result['Data']) > 0:\n",
    "            timeseries_data = result['Data'][0]['Data']\n",
    "            \n",
    "            # Each item in timeseries_data is a dict with Date and variable values\n",
    "            for data_point in timeseries_data:\n",
    "                row = {\n",
    "                    'field_id': field_id,\n",
    "                    'date': data_point.get('Date'),\n",
    "                    **{k: v for k, v in data_point.items() if k != 'Date'}  # All variables\n",
    "                }\n",
    "                rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "    \n",
    "# Main Loop\n",
    "all_dataset_results = {}\n",
    "async with aiohttp.ClientSession(raise_for_status=True, timeout=aiohttp.ClientTimeout(total=None)) as session:\n",
    "    for i, dataset in enumerate(DATASETS):\n",
    "\n",
    "        # Fetch dataset data\n",
    "        logger.info(f'{dataset}: starting...')\n",
    "        results = await fetch_data(dataset=dataset, dataset_index=i, session=session)\n",
    "        logger.info(f'{dataset}: fetched {len(results)} results')\n",
    "\n",
    "        # Write dataset data to dataframe\n",
    "        result_df = convert_results_to_pandas(results)\n",
    "\n",
    "        # Filter out unwanted months (years should already be capped to the specified range)\n",
    "        all_dataset_results[dataset] = result_df[pd.to_datetime(result_df['date']).dt.month.isin(MONTHS[i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677b746",
   "metadata": {},
   "source": [
    "### Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each dataset's DataFrame to a separate parquet file\n",
    "import os\n",
    "\n",
    "output_dir = 'data/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name, df in all_dataset_results.items():\n",
    "    output_file = f'{output_dir}/{dataset_name.lower()}_timeseries.parquet'\n",
    "    df.to_parquet(output_file, engine='pyarrow', compression='snappy', index=False)\n",
    "    logger.info(f'{dataset_name}: saved {len(df)} rows to {output_file}')\n",
    "    \n",
    "logger.info(f'All datasets saved to {output_dir}/')\n",
    "\n",
    "# Save a combined file with all datasets\n",
    "combined_df = pd.concat([\n",
    "    df.assign(dataset=dataset_name) \n",
    "    for dataset_name, df in all_dataset_results.items()\n",
    "], ignore_index=True)\n",
    "\n",
    "combined_file = f'{output_dir}/all_datasets_combined.parquet'\n",
    "combined_df.to_parquet(combined_file, engine='pyarrow', compression='snappy', index=False)\n",
    "logger.info(f'Combined: saved {len(combined_df)} rows to {combined_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e39982",
   "metadata": {},
   "source": [
    "## Processing\n",
    "In the final output, only the monthly mean and max ET values of each variable are needed. The data response from the timeseries API holds every single image from the time period, which is too granular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058116f8",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_dir = 'data/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "landsat_sr_raw = pd.read_parquet('data/output/landsat_sr_timeseries.parquet')\n",
    "openet_conus_raw = pd.read_parquet('data/output/openet_conus_timeseries.parquet')\n",
    "sentinel2_sr_raw = pd.read_parquet('data/output/sentinel2_sr_timeseries.parquet')\n",
    "raw_dfs = {\n",
    "    'landsat_sr': landsat_sr_raw, \n",
    "    'openet_conus': openet_conus_raw, \n",
    "    'sentinel2_sr': sentinel2_sr_raw\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499ec1a",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70abc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dfs = {}\n",
    "\n",
    "# Processing\n",
    "for dataset_name, raw_df in raw_dfs.items():\n",
    "    # Avoid sentinel value contamination\n",
    "    df = raw_df.replace(-9999, pd.NA)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    keys = ['field_id', 'date', 'year', 'month']\n",
    "    value_cols = [col for col in df.columns if col not in keys]\n",
    "\n",
    "    df[value_cols] = (\n",
    "        df[value_cols]\n",
    "        .apply(pd.to_numeric, errors='coerce')\n",
    "    )\n",
    "    \n",
    "    # Group by field_id and year_month, calculate mean and max\n",
    "    agg_df = df.groupby(['field_id', 'year', 'month'])[value_cols].agg(['mean', 'max']).reset_index()\n",
    "\n",
    "    # Flatten MultiIndex (cleaner column names)\n",
    "    agg_df.columns = [\n",
    "        f\"{col}_{stat}\" if stat else col\n",
    "        for col, stat in agg_df.columns\n",
    "    ]\n",
    "\n",
    "    # Round values to 4 decimals\n",
    "    stat_cols = [c for c in agg_df.columns if c not in ['field_id', 'year', 'month']]\n",
    "    agg_df[stat_cols] = agg_df[stat_cols].round(4)\n",
    "\n",
    "    # \n",
    "    \n",
    "    # Add finished data to final dictionary\n",
    "    processed_dfs[dataset_name] = agg_df\n",
    "\n",
    "# Write finished processed dataframes to files\n",
    "for dataset_name, processed_df in processed_dfs.items():\n",
    "\n",
    "    output_file = f'{output_dir}/{dataset_name}_timeseries_agg.parquet'\n",
    "    processed_df.to_parquet(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c2d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
